{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Examples for Beginners\n",
    "\n",
    "This Jupyter Notebook provides three introductory Python examples for Machine Learning (ML) tasks. We'll cover:\n",
    "1.  **PyTorch**: Simple Linear Regression\n",
    "2.  **TensorFlow**: Simple Neural Network for Classification\n",
    "3.  **OpenAI API**: Asking a static question to GPT-4o\n",
    "\n",
    "Each example is broken down into steps with explanations tailored for those new to ML engineering and with a basic understanding of Python. We will also explore the \"Practice Variations\" suggested in the source document.\n",
    "\n",
    "## Environment & Dependencies\n",
    "\n",
    "This notebook is designed to be run in a Python 3 environment. The examples use common ML libraries.\n",
    "\n",
    "**OS & GPU Context:**\n",
    "While these examples can run on various operating systems, this notebook was created with **Ubuntu 22.04** and an **NVIDIA RTX 3090 (24GB VRAM)** in mind. For PyTorch and TensorFlow, having a compatible NVIDIA GPU and the necessary CUDA drivers installed will allow for GPU acceleration, which can significantly speed up training for larger models. However, these specific examples are small enough to run efficiently on a CPU as well.\n",
    "\n",
    "**Installation:**\n",
    "You can install the necessary libraries using pip. Open your terminal or command prompt and run:\n",
    "```bash\n",
    "pip install torch torchvision torchaudio tensorflow scikit-learn openai numpy matplotlib\n",
    "```\n",
    "*(Note: `matplotlib` is added for potential visualizations, though not strictly required by the base examples.)*\n",
    "\n",
    "Ensure your Python environment is set up correctly before proceeding. For GPU support with PyTorch and TensorFlow, you might need to install specific versions compatible with your CUDA toolkit. Please refer to their official installation guides for detailed instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c3ee45",
   "metadata": {},
   "source": [
    "## Checking GPU Availability Using both PyTorch and TensorFlow\n",
    "\n",
    "### PyTorch Test\n",
    "#### Output should look like:\n",
    "```bash\n",
    "PyTorch CUDA version: 12.6\n",
    "PyTorch cuDNN version: 90800\n",
    "Number of GPUs available: 2\n",
    "GPU Name: NVIDIA GeForce RTX 3090\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1338b6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch CUDA version: 12.6\n",
      "PyTorch cuDNN version: 90800\n",
      "Number of GPUs available: 2\n",
      "GPU Name: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"PyTorch CUDA version: {torch.version.cuda}\")\n",
    "    # You can also check the cudnn version PyTorch is linked with\n",
    "    print(f\"PyTorch cuDNN version: {torch.backends.cudnn.version()}\")\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\") # Gets name of first GPU\n",
    "else:\n",
    "    print(\"PyTorch: CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94ce908",
   "metadata": {},
   "source": [
    "TensorFlow \n",
    "#### Output should look like:\n",
    "```bash\n",
    "2025-05-11 20:50:25.015083: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
    "2025-05-11 20:50:25.020710: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
    "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
    "E0000 00:00:1747011025.027882   13580 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
    "E0000 00:00:1747011025.030095   13580 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
    "2025-05-11 20:50:25.037501: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
    "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
    "TensorFlow version: 2.18.0\n",
    "TensorFlow is using 2 GPU(s).\n",
    "TensorFlow built with CUDA version: 12.6\n",
    "TensorFlow built with cuDNN version: 9\n",
    "```\n",
    "#### Output explanation:\n",
    "1. The run logs show that TensorFlow is now imported successfully and is running using 2 GPUs. The lines about registering cuFFT, cuDNN, and cuBLAS are warnings that arise when TensorFlow detects that these libraries have already been registered. They are expected in multi‚ÄêGPU environments and do not indicate a critical error.\n",
    "2. If you prefer to suppress these verbose logs, you can set the environment variable TF_CPP_MIN_LOG_LEVEL to 2 before importing TensorFlow.\n",
    "\n",
    "    ```python\n",
    "    import os\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress INFO and WARNING messages from TensorFlow\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f44ac6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n",
      "TensorFlow is using 2 GPU(s).\n",
      "TensorFlow built with CUDA version: 12.6\n",
      "TensorFlow built with cuDNN version: 9\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import tensorflow as tf\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Error: tensorflow module not found. Please install it via 'pip install tensorflow'\")\n",
    "    raise\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print(f\"TensorFlow is using {len(gpu_devices)} GPU(s).\")\n",
    "    # To get more details about the CUDA version TF was built with:\n",
    "    build_info = tf.sysconfig.get_build_info()\n",
    "    print(f\"TensorFlow built with CUDA version: {build_info.get('cuda_version', 'N/A')}\")\n",
    "    print(f\"TensorFlow built with cuDNN version: {build_info.get('cudnn_version', 'N/A')}\")\n",
    "else:\n",
    "    print(\"TensorFlow: No GPU devices found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be72c9d7",
   "metadata": {},
   "source": [
    "## 1. PyTorch Example: Simple Linear Regression\n",
    "\n",
    "This example demonstrates how to build a basic linear regression model using PyTorch. Linear regression is a fundamental algorithm in machine learning used to model the linear relationship between a dependent variable (y) and one or more independent variables (x). Our goal is to predict `y = 2x + 1` based on some generated data with a bit of noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a380df",
   "metadata": {},
   "source": [
    "### 1.1 Importing Libraries\n",
    "\n",
    "First, we import the necessary libraries:\n",
    "-   `torch`: The main PyTorch library.\n",
    "-   `torch.nn`: A submodule in PyTorch that contains building blocks for creating neural networks (like layers, loss functions, etc.). `nn` stands for Neural Network.\n",
    "-   `numpy`: A popular library for numerical computations in Python, often used for creating and manipulating arrays (which we'll convert to PyTorch tensors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc85ec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # For plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebd3b55",
   "metadata": {},
   "source": [
    "### 1.2 Generating Synthetic Data\n",
    "\n",
    "We need data to train our model. Since we want to model `y = 2x + 1`, we'll generate some `x` values and calculate the corresponding `y` values, adding a small amount of random noise to make it more realistic.\n",
    "\n",
    "-   `np.linspace(0, 10, 100)`: Creates 100 evenly spaced numbers between 0 and 10. This will be our `x`.\n",
    "-   `.reshape(-1, 1)`: Changes the shape of `x` to be a 2D array with 100 rows and 1 column. PyTorch models usually expect inputs in this format.\n",
    "-   `astype(np.float32)`: Converts the data type to 32-bit floating point numbers, which is standard for PyTorch tensors.\n",
    "-   `y = 2 * x + 1 + np.random.normal(0, 0.1, x.shape)`: Calculates `y` using our linear equation and adds some Gaussian noise (mean 0, standard deviation 0.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc253b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "x_numpy = np.linspace(0, 10, 100).reshape(-1, 1).astype(np.float32)\n",
    "y_numpy = 2 * x_numpy + 1 + np.random.normal(0, 0.1, x_numpy.shape).astype(np.float32)\n",
    "\n",
    "# Let's visualize the data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x_numpy, y_numpy, label='Original data with noise', color='blue', s=10) # s is marker size\n",
    "plt.plot(x_numpy, 2 * x_numpy + 1, label='True underlying relationship (y=2x+1)', color='red', linestyle='--')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Synthetic Data for Linear Regression')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b465e47",
   "metadata": {},
   "source": [
    "### 1.3 Converting Data to PyTorch Tensors\n",
    "\n",
    "PyTorch models work with PyTorch tensors, not NumPy arrays directly. So, we convert our NumPy arrays `x_numpy` and `y_numpy` into tensors.\n",
    "\n",
    "-   `torch.from_numpy()`: This function does the conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ccc6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "x_tensor = torch.from_numpy(x_numpy)\n",
    "y_tensor = torch.from_numpy(y_numpy)\n",
    "\n",
    "print(f\"Shape of x_tensor: {x_tensor.shape}\")\n",
    "print(f\"Shape of y_tensor: {y_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168a710f",
   "metadata": {},
   "source": [
    "### 1.4 Defining the Linear Regression Model\n",
    "\n",
    "We define our model as a Python class that inherits from `nn.Module`. This is the standard way to create models in PyTorch.\n",
    "\n",
    "-   `__init__(self)`: The constructor. Here, we define the layers of our model. \n",
    "    -   `super(LinearRegression, self).__init__()`: Calls the constructor of the parent class (`nn.Module`). This is necessary.\n",
    "    -   `self.linear = nn.Linear(1, 1)`: This creates a linear layer. \n",
    "        -   The first `1` means the input to this layer has 1 feature (our `x` value).\n",
    "        -   The second `1` means the output of this layer has 1 feature (our predicted `y` value).\n",
    "        -   This layer will learn a weight (slope) and a bias (intercept) for the equation `y = weight * x + bias`.\n",
    "-   `forward(self, x)`: This method defines how the input `x` flows through the model to produce an output. Here, it simply passes `x` through our linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dcaa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple linear regression model\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        # Input features: 1 (x), Output features: 1 (y_predicted)\n",
    "        self.linear = nn.Linear(in_features=1, out_features=1)\n",
    "      \n",
    "    def forward(self, x):\n",
    "        # This defines the computation performed at every call.\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b0d561",
   "metadata": {},
   "source": [
    "### 1.5 Initializing Model, Loss Function, and Optimizer\n",
    "\n",
    "-   **Model**: `model = LinearRegressionModel()`: We create an instance of our model class.\n",
    "-   **Loss Function**: `criterion = nn.MSELoss()`: The loss function measures how different our model's predictions are from the actual `y` values. \n",
    "    -   `MSELoss` stands for Mean Squared Error. It calculates the average of the squares of the differences between predictions and actual values. This is a common loss function for regression tasks.\n",
    "-   **Optimizer**: `optimizer = torch.optim.SGD(model.parameters(), lr=0.01)`: The optimizer is responsible for updating the model's parameters (weights and biases) to minimize the loss.\n",
    "    -   `torch.optim.SGD`: Stochastic Gradient Descent. A common optimization algorithm.\n",
    "    -   `model.parameters()`: Tells the optimizer which parameters (weights and biases of our `self.linear` layer) it needs to update.\n",
    "    -   `lr=0.01`: Learning rate. This controls how big the steps are when updating parameters. A smaller learning rate means smaller steps and potentially longer training, but can be more stable. A larger learning rate can speed up training but might overshoot the optimal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175131e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "model = LinearRegressionModel()\n",
    "criterion = nn.MSELoss() # Mean Squared Error Loss\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) # Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447f3def",
   "metadata": {},
   "source": [
    "### 1.6 Training Loop\n",
    "\n",
    "This is where the model learns. We repeat the process for a certain number of `epochs`. An epoch is one complete pass through the entire training dataset.\n",
    "\n",
    "Inside the loop:\n",
    "1.  `y_pred = model(x_tensor)`: Forward pass. Get predictions from the model using the current `x_tensor`.\n",
    "2.  `loss = criterion(y_pred, y_tensor)`: Calculate the loss by comparing the predictions (`y_pred`) with the actual values (`y_tensor`).\n",
    "3.  `optimizer.zero_grad()`: Clear old gradients. Gradients are values that indicate the direction to adjust parameters to reduce loss. We need to clear them before calculating new ones for the current step, otherwise they accumulate.\n",
    "4.  `loss.backward()`: Backward pass. Calculate the gradients of the loss with respect to the model's parameters. This is where PyTorch's automatic differentiation (autograd) shines.\n",
    "5.  `optimizer.step()`: Update parameters. The optimizer adjusts the model's parameters using the calculated gradients and the learning rate.\n",
    "6.  `if epoch % 20 == 0:`: Print the loss every 20 epochs to monitor training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5954f698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "epochs = 100\n",
    "losses = # To store loss values for plotting\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 1. Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x_tensor)\n",
    "    \n",
    "    # 2. Compute loss\n",
    "    loss = criterion(y_pred, y_tensor)\n",
    "    losses.append(loss.item()) # .item() gets the scalar value of the loss\n",
    "    \n",
    "    # 3. Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()  # Clears existing gradients\n",
    "    loss.backward()      # Computes gradients of the loss w.r.t. parameters\n",
    "    optimizer.step()       # Updates parameters based on gradients\n",
    "      \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Print Learned Parameters and Visualize Results\n",
    "\n",
    "After training, our model's `linear` layer has learned a weight (slope) and a bias (intercept). We expect these to be close to 2 and 1, respectively, since our original data was generated from `y = 2x + 1`.\n",
    "\n",
    "-   `model.linear.weight.item()`: Gets the learned weight (slope).\n",
    "-   `model.linear.bias.item()`: Gets the learned bias (intercept).\n",
    "\n",
    "We also plot the original data, the true line, and the line our model learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print learned parameters\n",
    "learned_weight = model.linear.weight.item()\n",
    "learned_bias = model.linear.bias.item()\n",
    "print(f\"\\nLearned equation: y = {learned_weight:.4f}x + {learned_bias:.4f}\")\n",
    "\n",
    "# Plot the training loss\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(range(epochs), losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Visualize the learned line\n",
    "model.eval() # Set the model to evaluation mode (important for some layers like dropout, batchnorm)\n",
    "with torch.no_grad(): # Disable gradient calculation for inference\n",
    "    predicted_line = model(x_tensor).detach().numpy() # Get predictions and convert to numpy\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x_numpy, y_numpy, label='Original data', color='blue', s=10)\n",
    "plt.plot(x_numpy, 2 * x_numpy + 1, label='True line (y=2x+1)', color='red', linestyle='--')\n",
    "plt.plot(x_numpy, predicted_line, label='Learned line', color='green')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear Regression Result')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 PyTorch Practice Variation: Different Optimizer and Loss Function\n",
    "\n",
    "The original example uses Stochastic Gradient Descent (`SGD`) as the optimizer and Mean Squared Error (`MSELoss`) as the loss function. Let's try changing these:\n",
    "\n",
    "-   **Optimizer**: We can use `Adam` (Adaptive Moment Estimation), which is often a good default optimizer and can converge faster or achieve better results than SGD for some problems. It adapts the learning rate for each parameter.\n",
    "-   **Loss Function**: We can use `L1Loss` (Mean Absolute Error or MAE). Instead of squaring the differences between predicted and actual values, `L1Loss` takes the absolute difference. This can make the model less sensitive to outliers compared to `MSELoss`.\n",
    "\n",
    "We'll repeat the process with these changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize model (to reset learned parameters)\n",
    "model_variation = LinearRegressionModel()\n",
    "\n",
    "# Variation: Use Adam optimizer and L1Loss\n",
    "criterion_variation = nn.L1Loss()  # Mean Absolute Error\n",
    "optimizer_variation = torch.optim.Adam(model_variation.parameters(), lr=0.01) # Adam optimizer\n",
    "\n",
    "print(\"Training with Adam optimizer and L1Loss:\\n\")\n",
    "losses_variation = []\n",
    "\n",
    "# Training loop for the variation\n",
    "for epoch in range(epochs): # Using the same number of epochs\n",
    "    y_pred_variation = model_variation(x_tensor)\n",
    "    loss_variation = criterion_variation(y_pred_variation, y_tensor)\n",
    "    losses_variation.append(loss_variation.item())\n",
    "    \n",
    "    optimizer_variation.zero_grad()\n",
    "    loss_variation.backward()\n",
    "    optimizer_variation.step()\n",
    "      \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss (L1): {loss_variation.item():.4f}\")\n",
    "\n",
    "# Print learned parameters for the variation\n",
    "learned_weight_var = model_variation.linear.weight.item()\n",
    "learned_bias_var = model_variation.linear.bias.item()\n",
    "print(f\"\\nLearned equation (variation): y = {learned_weight_var:.4f}x + {learned_bias_var:.4f}\")\n",
    "\n",
    "# Plot the training loss for the variation\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(range(epochs), losses_variation, label='L1 Loss with Adam')\n",
    "plt.plot(range(epochs), losses, label='MSE Loss with SGD (original)', linestyle='--') # Plot original for comparison\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Comparison: Variation vs Original')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Visualize the learned line for the variation\n",
    "model_variation.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_line_var = model_variation(x_tensor).detach().numpy()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x_numpy, y_numpy, label='Original data', color='blue', s=10)\n",
    "plt.plot(x_numpy, 2 * x_numpy + 1, label='True line (y=2x+1)', color='red', linestyle='--')\n",
    "plt.plot(x_numpy, predicted_line_var, label='Learned line (Adam, L1Loss)', color='purple')\n",
    "plt.plot(x_numpy, predicted_line, label='Learned line (SGD, MSELoss) - original', color='green', linestyle=':')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear Regression Result: Variation vs Original')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can observe how the choice of optimizer and loss function might affect the training process (e.g., speed of convergence, final loss value) and the resulting learned parameters. Adam often converges faster than basic SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TensorFlow Example: Simple Neural Network for Classification\n",
    "\n",
    "This example uses TensorFlow with its Keras API to create a simple neural network for binary classification. Classification means assigning input data to one of several categories. Binary classification means there are only two categories (e.g., yes/no, spam/not-spam).\n",
    "\n",
    "We'll use synthetic (artificially generated) data for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Importing Libraries\n",
    "\n",
    "-   `tensorflow` as `tf`: The main TensorFlow library.\n",
    "-   `numpy`: For numerical operations, though TensorFlow has its own tensor operations too.\n",
    "-   `sklearn.datasets.make_classification`: A utility from Scikit-learn to generate a random n-class classification problem. This is great for quickly creating data to test models.\n",
    "-   `sklearn.model_selection.train_test_split`: A utility to split data into training and testing sets. We train the model on the training set and then evaluate its performance on the unseen testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler # For feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Generating Synthetic Classification Data\n",
    "\n",
    "-   `make_classification(...)`: Generates the data.\n",
    "    -   `n_samples=1000`: We want 1000 data points (samples).\n",
    "    -   `n_features=4`: Each sample will have 4 input features.\n",
    "    -   `n_classes=2`: There are 2 output classes (binary classification).\n",
    "    -   `random_state=42`: Ensures that we get the same random data every time we run the code, making results reproducible. `42` is just a commonly used number.\n",
    "-   `X` will contain the features (our input data), and `y` will contain the corresponding class labels (0 or 1).\n",
    "-   `train_test_split(X, y, test_size=0.2, random_state=42)`: Splits the data. \n",
    "    -   `test_size=0.2`: Reserves 20% of the data for the test set, and the remaining 80% for the training set.\n",
    "\n",
    "**Feature Scaling**:\n",
    "It's often good practice to scale input features, especially for neural networks. This helps the optimizer work more effectively. We'll use `StandardScaler` from Scikit-learn, which standardizes features by removing the mean and scaling to unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic binary classification data\n",
    "X, y = make_classification(n_samples=1000, n_features=4, n_informative=2, n_redundant=0, \n",
    "                           n_clusters_per_class=1, n_classes=2, random_state=42)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train) # Fit on training data and transform it\n",
    "X_test = scaler.transform(X_test)     # Transform test data using the same scaler\n",
    "\n",
    "print(f\"Shape of X_train: {X_train.shape}, Shape of y_train: {y_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}, Shape of y_test: {y_test.shape}\")\n",
    "print(f\"First 5 samples of X_train (scaled):\\n{X_train[:5]}\")\n",
    "print(f\"First 5 labels of y_train:\\n{y_train[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Defining the Neural Network Model\n",
    "\n",
    "We use Keras's `Sequential` API, which is a straightforward way to build models by stacking layers one after the other.\n",
    "\n",
    "-   `tf.keras.Sequential([...])`: Creates a sequential model.\n",
    "-   `tf.keras.layers.Dense(16, activation='relu', input_shape=(4,))`: The first hidden layer.\n",
    "    -   `Dense`: A fully connected layer, meaning each neuron in this layer is connected to every neuron in the previous layer (or input).\n",
    "    -   `16`: The number of neurons (or units) in this layer.\n",
    "    -   `activation='relu'`: The activation function. 'ReLU' (Rectified Linear Unit) is a common choice for hidden layers. It outputs the input directly if it's positive, otherwise, it outputs zero.\n",
    "    -   `input_shape=(4,)`: Specifies the shape of the input for the first layer only. Our data has 4 features.\n",
    "-   `tf.keras.layers.Dense(8, activation='relu')`: The second hidden layer with 8 neurons and ReLU activation.\n",
    "-   `tf.keras.layers.Dense(1, activation='sigmoid')`: The output layer.\n",
    "    -   `1`: One neuron because this is binary classification (it will output a single value between 0 and 1).\n",
    "    -   `activation='sigmoid'`: The sigmoid activation function squashes the output to a range between 0 and 1. This can be interpreted as the probability of belonging to class 1. For multi-class classification, you'd typically use `softmax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple neural network\n",
    "model_tf = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation='relu', input_shape=(X_train.shape[1],)), # Input shape from X_train\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid') # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Display the model's architecture\n",
    "model_tf.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Compiling the Model\n",
    "\n",
    "Before training, we need to configure the learning process using the `compile` method.\n",
    "\n",
    "-   `optimizer='adam'`: Specifies the optimizer. 'Adam' is another popular and effective optimization algorithm.\n",
    "-   `loss='binary_crossentropy'`: Specifies the loss function. `binary_crossentropy` is suitable for binary classification problems where the output is a probability (due to the sigmoid activation in the last layer).\n",
    "-   `metrics=['accuracy']`: Specifies metrics to monitor during training and evaluation. Here, we want to see the 'accuracy' (the proportion of correctly classified samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model_tf.compile(optimizer='adam', \n",
    "                 loss='binary_crossentropy', \n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Training the Model\n",
    "\n",
    "Now we train the model using the `fit` method.\n",
    "\n",
    "-   `X_train, y_train`: The training data (features and labels).\n",
    "-   `epochs=10`: The number of times the model will see the entire training dataset.\n",
    "-   `batch_size=32`: The number of samples processed before the model's parameters are updated. The training data is divided into batches.\n",
    "-   `validation_split=0.2`: Reserves 20% of the *training data* to be used as validation data. The model's performance on this validation data is monitored during training, which can help detect overfitting (when the model performs well on training data but poorly on unseen data).\n",
    "-   `verbose=1`: How much information to display during training (1 usually means a progress bar and metrics per epoch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"\\nTraining the TensorFlow model...\")\n",
    "history = model_tf.fit(X_train, y_train, \n",
    "                         epochs=10, \n",
    "                         batch_size=32, \n",
    "                         validation_split=0.2, # Use part of training data for validation\n",
    "                         verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Evaluating the Model\n",
    "\n",
    "After training, we evaluate the model's performance on the test set (`X_test`, `y_test`), which it has never seen before.\n",
    "\n",
    "-   `model.evaluate(X_test, y_test, verbose=0)`: Calculates the loss and any specified metrics (like accuracy) on the test data. `verbose=0` means it won't print progress during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model_tf.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "if 'accuracy' in history.history and 'val_accuracy' in history.history:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Accuracy/loss history not fully available for plotting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 TensorFlow Practice Variation: Add Dropout Layers\n",
    "\n",
    "**Dropout** is a regularization technique used to prevent overfitting in neural networks. Overfitting happens when a model learns the training data too well, including its noise, and performs poorly on new, unseen data.\n",
    "\n",
    "During training, a dropout layer randomly sets a fraction of input units to 0 at each update. This helps to make the network more robust and prevents neurons from co-adapting too much.\n",
    "\n",
    "-   `tf.keras.layers.Dropout(0.2)`: Adds a dropout layer. `0.2` means that 20% of the input units will be randomly set to 0 during each update in the training phase. Dropout is only active during training, not during evaluation or inference.\n",
    "\n",
    "Let's modify our model to include dropout layers after each Dense hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a neural network with Dropout layers\n",
    "model_tf_variation = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dropout(0.3),  # Dropout layer after the first Dense layer (30% dropout rate)\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),  # Dropout layer after the second Dense layer (20% dropout rate)\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Display the new model's architecture\n",
    "model_tf_variation.summary()\n",
    "\n",
    "# Compile the variation model\n",
    "model_tf_variation.compile(optimizer='adam', \n",
    "                           loss='binary_crossentropy', \n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "# Train the variation model\n",
    "print(\"\\nTraining the TensorFlow model with Dropout...\")\n",
    "history_variation = model_tf_variation.fit(X_train, y_train, \n",
    "                                           epochs=10, # Using fewer epochs to see effect quickly, can increase\n",
    "                                           batch_size=32, \n",
    "                                           validation_split=0.2, \n",
    "                                           verbose=1)\n",
    "\n",
    "# Evaluate the variation model\n",
    "test_loss_var, test_accuracy_var = model_tf_variation.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nTest Loss (Variation with Dropout): {test_loss_var:.4f}\")\n",
    "print(f\"Test Accuracy (Variation with Dropout): {test_accuracy_var:.4f}\")\n",
    "\n",
    "# Plot training & validation accuracy and loss for the variation model\n",
    "if 'accuracy' in history_variation.history and 'val_accuracy' in history_variation.history:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history_variation.history['accuracy'], label='Train Acc (Dropout)')\n",
    "    plt.plot(history_variation.history['val_accuracy'], label='Val Acc (Dropout)')\n",
    "    if 'accuracy' in history.history: # Original model's history\n",
    "        plt.plot(history.history['accuracy'], label='Train Acc (Original)', linestyle=':')\n",
    "        plt.plot(history.history['val_accuracy'], label='Val Acc (Original)', linestyle=':')\n",
    "    plt.title('Model Accuracy Comparison')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history_variation.history['loss'], label='Train Loss (Dropout)')\n",
    "    plt.plot(history_variation.history['val_loss'], label='Val Loss (Dropout)')\n",
    "    if 'loss' in history.history: # Original model's history\n",
    "        plt.plot(history.history['loss'], label='Train Loss (Original)', linestyle=':')\n",
    "        plt.plot(history.history['val_loss'], label='Val Loss (Original)', linestyle=':')\n",
    "    plt.title('Model Loss Comparison')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Accuracy/loss history for variation model not fully available for plotting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With dropout, you might see that the training accuracy is a bit lower or increases slower, but the validation accuracy might be better or the gap between training and validation accuracy might be smaller, indicating less overfitting. The effect of dropout is usually more pronounced on more complex models and datasets where overfitting is a bigger risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. OpenAI API Example: Ask GPT-4o a Static Question\n",
    "\n",
    "This example shows how to use the OpenAI API to send a question to the GPT-4o model and get a response. This involves making an HTTP request to OpenAI's servers.\n",
    "\n",
    "**Important Note**: To run this example, you need an OpenAI API key. \n",
    "1.  If you don't have one, sign up at [OpenAI's website](https://platform.openai.com/).\n",
    "2.  Replace `'YOUR_API_KEY'` in the code below with your actual API key.\n",
    "3.  **Keep your API key secret!** Do not share it publicly or commit it to version control.\n",
    "\n",
    "You will also need an active internet connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Importing the OpenAI Library\n",
    "\n",
    "-   `openai`: The official Python library for interacting with the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to install the openai library if you haven't already: !pip install openai\n",
    "import openai\n",
    "from openai import OpenAI # Recommended way to import client\n",
    "import os # To potentially use environment variables for API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Initializing the OpenAI Client\n",
    "\n",
    "We create an instance of the `OpenAI` client, providing our API key.\n",
    "\n",
    "**Security Best Practice**: It's better to store your API key as an environment variable rather than hardcoding it. \n",
    "For example, you could set an environment variable `OPENAI_API_KEY` and then use `api_key=os.environ.get('OPENAI_API_KEY')`.\n",
    "For simplicity in this example, we'll show the direct replacement method, but **be mindful of security**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- IMPORTANT: REPLACE 'YOUR_API_KEY' WITH YOUR ACTUAL KEY --- \n",
    "# api_key_value = \"YOUR_API_KEY\" \n",
    "# For safety, it's better to use an environment variable or input prompt:\n",
    "try:\n",
    "    # Attempt to get API key from environment variable first\n",
    "    api_key_value = os.environ[\"OPENAI_API_KEY\"]\n",
    "    if not api_key_value:\n",
    "        raise KeyError\n",
    "    print(\"Using API key from environment variable OPENAI_API_KEY.\")\n",
    "except KeyError:\n",
    "    print(\"OPENAI_API_KEY environment variable not found.\")\n",
    "    # Fallback: prompt user for API key if not in environment\n",
    "    # In a real notebook, you might use input() but be careful with visibility\n",
    "    api_key_value = \"YOUR_API_KEY_PLACEHOLDER\" # Replace this or use input()\n",
    "    if api_key_value == \"YOUR_API_KEY_PLACEHOLDER\":\n",
    "         print(\"Please replace 'YOUR_API_KEY_PLACEHOLDER' with your actual key or set the OPENAI_API_KEY environment variable.\")\n",
    "         # You might want to raise an error or skip this section if no key is provided\n",
    "         # For now, we'll let it proceed, but it will fail if the key is invalid.\n",
    "\n",
    "client = None\n",
    "if api_key_value and api_key_value != \"YOUR_API_KEY_PLACEHOLDER\":\n",
    "    try:\n",
    "        client = OpenAI(api_key=api_key_value)\n",
    "        print(\"OpenAI client initialized.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing OpenAI client: {e}\")\n",
    "else:\n",
    "    print(\"OpenAI client NOT initialized. Please provide a valid API key.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Defining the Question and Making the API Request\n",
    "\n",
    "-   `question`: The question we want to ask the model.\n",
    "-   `client.chat.completions.create(...)`: This is the method used to interact with chat-based models like GPT-3.5-turbo, GPT-4, and GPT-4o.\n",
    "    -   `model=\"gpt-4o\"`: Specifies which model to use. \"gpt-4o\" is OpenAI's latest flagship model (as of this writing).\n",
    "    -   `messages=[...]`: A list of message objects that form the conversation history.\n",
    "        -   `{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}`: The system message sets the context or behavior for the assistant.\n",
    "        -   `{\"role\": \"user\", \"content\": question}`: The user's message (our question).\n",
    "    -   `max_tokens=50`: Limits the maximum length of the generated response. One token is roughly 4 characters of text for English.\n",
    "    -   `temperature=0.7`: Controls the randomness of the output. \n",
    "        -   Higher values (e.g., 0.7-1.0) make the output more random and creative.\n",
    "        -   Lower values (e.g., 0.1-0.3) make it more focused and deterministic.\n",
    "        -   A value of 0 would make it mostly deterministic, but not always perfectly so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if client: # Only proceed if client was initialized\n",
    "    # Define a static question\n",
    "    question = \"What is the capital city of France?\"\n",
    "\n",
    "    try:\n",
    "        # Make a request to GPT-4o\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a concise and helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ],\n",
    "            max_tokens=50, # Maximum number of tokens in the response\n",
    "            temperature=0.7  # Controls randomness: 0.0 (deterministic) to 2.0 (very random)\n",
    "        )\n",
    "\n",
    "        # Extract and print the response\n",
    "        # The response structure is a choice object, we need to access the message content\n",
    "        answer = response.choices[0].message.content\n",
    "        print(f\"\\nQuestion: {question}\")\n",
    "        print(f\"Answer: {answer.strip()}\") # .strip() removes leading/trailing whitespace\n",
    "\n",
    "    except openai.APIConnectionError as e:\n",
    "        print(f\"OpenAI API request failed to connect: {e}\")\n",
    "    except openai.RateLimitError as e:\n",
    "        print(f\"OpenAI API request exceeded rate limit: {e}\")\n",
    "    except openai.AuthenticationError as e:\n",
    "        print(f\"OpenAI API authentication failed. Check your API key: {e}\")\n",
    "    except openai.APIError as e:\n",
    "        print(f\"OpenAI API returned an API Error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "else:\n",
    "    print(\"\\nSkipping OpenAI API call as the client was not initialized (API key missing or invalid).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 OpenAI API Practice Variation: Different Question and Parameters\n",
    "\n",
    "Let's try asking a different question and adjusting the `max_tokens` and `temperature` parameters to see how they affect the output.\n",
    "\n",
    "-   **New Question**: \"Explain the concept of a 'tensor' in machine learning in one sentence.\"\n",
    "-   **`max_tokens`**: We might increase this slightly if we expect a slightly longer answer, or keep it controlled for brevity.\n",
    "-   **`temperature`**: Let's try a lower temperature (e.g., 0.2) for a more deterministic, factual answer, and a higher temperature (e.g., 0.9) for a more creative (though potentially less accurate for factual questions) response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if client: # Only proceed if client was initialized\n",
    "    new_question = \"Explain the concept of a 'tensor' in machine learning in one or two simple sentences.\"\n",
    "\n",
    "    print(f\"\\n--- OpenAI Variation 1: Different Question, Lower Temperature ---\")\n",
    "    try:\n",
    "        response_var1 = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful and concise AI assistant, skilled at explaining complex topics simply.\"},\n",
    "                {\"role\": \"user\", \"content\": new_question}\n",
    "            ],\n",
    "            max_tokens=80,  # Slightly more tokens allowed\n",
    "            temperature=0.2   # Lower temperature for more focused response\n",
    "        )\n",
    "        answer_var1 = response_var1.choices[0].message.content\n",
    "        print(f\"Question: {new_question}\")\n",
    "        print(f\"Answer (temp 0.2): {answer_var1.strip()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during OpenAI API call (Variation 1): {e}\")\n",
    "\n",
    "    print(f\"\\n--- OpenAI Variation 2: Same Question, Higher Temperature ---\")\n",
    "    try:\n",
    "        response_var2 = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful and somewhat creative AI assistant.\"}, \n",
    "                {\"role\": \"user\", \"content\": new_question} \n",
    "            ],\n",
    "            max_tokens=80,\n",
    "            temperature=0.9   # Higher temperature for more creative/varied response\n",
    "        )\n",
    "        answer_var2 = response_var2.choices[0].message.content\n",
    "        print(f\"Question: {new_question}\")\n",
    "        print(f\"Answer (temp 0.9): {answer_var2.strip()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during OpenAI API call (Variation 2): {e}\")\n",
    "else:\n",
    "    print(\"\\nSkipping OpenAI API variations as the client was not initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By changing the `temperature`, you'll likely get different phrasings or slightly different explanations for the same question. Lower temperatures are good for factual recall, while higher temperatures can be used for more generative or brainstorming tasks (but be cautious about factual accuracy at very high temperatures)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Further Learning\n",
    "\n",
    "These examples provide a basic introduction to using PyTorch, TensorFlow, and the OpenAI API for common machine learning tasks. \n",
    "\n",
    "**Key Takeaways for Learning & Interviews:**\n",
    "\n",
    "* **Understand the Core Concepts:**\n",
    "    * **PyTorch**: Tensors, `nn.Module` for model building, autograd for gradients, optimizers, and the training loop structure.\n",
    "    * **TensorFlow (Keras)**: The `Sequential` or Functional API for model building, layers, compilation (optimizer, loss, metrics), and the `fit`/`evaluate` workflow.\n",
    "    * **OpenAI API**: Client initialization, structuring messages for chat completions, understanding parameters like `model`, `temperature`, and `max_tokens`, and how to parse the response.\n",
    "* **Practice Explaining Code**: Be able to walk through each part of the code and explain *why* certain choices were made (e.g., why use `relu` vs `sigmoid`, why `binary_crossentropy` for this problem).\n",
    "* **Experiment with Variations**: Actively try modifying these examples. Change architectures, try different optimizers/loss functions, explore other datasets, or ask more complex questions to the OpenAI API. This is the best way to solidify your understanding.\n",
    "* **Setup and Dependencies**: Know how to install the necessary libraries (`pip install ...`) and be aware of potential environment issues (e.g., Python versions, CUDA for GPU).\n",
    "* **Error Handling**: For API calls (like OpenAI), understand common errors (authentication, rate limits, connection issues) and how you might handle them in a real application.\n",
    "\n",
    "This notebook is just a starting point. The field of ML is vast and constantly evolving. Continue exploring documentation, tutorials, and building your own projects to deepen your knowledge!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (classification_models_py3.12)",
   "language": "python",
   "name": "classification_models_py3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
