{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment & Dependencies\n",
    "\n",
    "This notebook is designed to be run in a Python 3 environment and focuses on PyTorch for linear regression.\n",
    "\n",
    "**OS & GPU Context:**\n",
    "While this example can run on various operating systems, this notebook was created with **Ubuntu 22.04** and an **NVIDIA RTX 3090 (24GB VRAM)** in mind. For PyTorch, having a compatible NVIDIA GPU and the necessary CUDA drivers installed will allow for GPU acceleration, which can significantly speed up training for larger models. However, this specific example is small enough to run efficiently on a CPU as well.\n",
    "\n",
    "**Installation:**\n",
    "You can install the necessary libraries using pip. Open your terminal or command prompt and run:\n",
    "```bash\n",
    "pip install torch torchvision torchaudio numpy matplotlib\n",
    "```\n",
    "*(Note: `matplotlib` is added for potential visualizations, though not strictly required by the base examples.)*\n",
    "\n",
    "Ensure your Python environment is set up correctly before proceeding. For GPU support with PyTorch, you might need to install specific versions compatible with your CUDA toolkit. Please refer to PyTorch's official installation guide for detailed instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c3ee45",
   "metadata": {},
   "source": [
    "## Checking GPU Availability Using PyTorch\n",
    "\n",
    "### PyTorch Test\n",
    "#### Output should look like:\n",
    "```bash\n",
    "PyTorch CUDA version: 12.6\n",
    "PyTorch cuDNN version: 90800\n",
    "Number of GPUs available: 2\n",
    "GPU Name: NVIDIA GeForce RTX 3090\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1338b6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"PyTorch CUDA version: {torch.version.cuda}\")\n",
    "    # You can also check the cudnn version PyTorch is linked with\n",
    "    print(f\"PyTorch cuDNN version: {torch.backends.cudnn.version()}\")\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\") # Gets name of first GPU\n",
    "else:\n",
    "    print(\"PyTorch: CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be72c9d7",
   "metadata": {},
   "source": [
    "## 1. PyTorch: Simple Linear Regression\n",
    "\n",
    "This example demonstrates how to build a basic linear regression model using PyTorch. Linear regression is a fundamental algorithm in machine learning used to model the linear relationship between a dependent variable (y) and one or more independent variables (x). Our goal is to predict `y = 2x + 1` based on some generated data with a bit of noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a380df",
   "metadata": {},
   "source": [
    "### 1.1 Importing Libraries\n",
    "\n",
    "First, we import the necessary libraries:\n",
    "-   `torch`: The main PyTorch library.\n",
    "-   `torch.nn`: A submodule in PyTorch that contains building blocks for creating neural networks (like layers, loss functions, etc.). `nn` stands for Neural Network.\n",
    "-   `numpy`: A popular library for numerical computations in Python, often used for creating and manipulating arrays (which we'll convert to PyTorch tensors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc85ec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # For plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebd3b55",
   "metadata": {},
   "source": [
    "### 1.2 Generating Synthetic Data\n",
    "\n",
    "We need data to train our model. Since we want to model `y = 2x + 1`, we'll generate some `x` values and calculate the corresponding `y` values, adding a small amount of random noise to make it more realistic.\n",
    "\n",
    "-   `np.linspace(0, 10, 100)`: Creates 100 evenly spaced numbers between 0 and 10. This will be our `x`.\n",
    "-   `.reshape(-1, 1)`: Changes the shape of `x` to be a 2D array with 100 rows and 1 column. PyTorch models usually expect inputs in this format.\n",
    "-   `astype(np.float32)`: Converts the data type to 32-bit floating point numbers, which is standard for PyTorch tensors.\n",
    "-   `y = 2 * x + 1 + np.random.normal(0, 0.1, x.shape)`: Calculates `y` using our linear equation and adds some Gaussian noise (mean 0, standard deviation 0.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc253b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "x_numpy = np.linspace(0, 10, 100).reshape(-1, 1).astype(np.float32)\n",
    "y_numpy = 2 * x_numpy + 1 + np.random.normal(0, 0.1, x_numpy.shape).astype(np.float32)\n",
    "\n",
    "# Let's visualize the data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x_numpy, y_numpy, label='Original data with noise', color='blue', s=10) # s is marker size\n",
    "plt.plot(x_numpy, 2 * x_numpy + 1, label='True underlying relationship (y=2x+1)', color='red', linestyle='--')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Synthetic Data for Linear Regression')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b465e47",
   "metadata": {},
   "source": [
    "### 1.3 Converting Data to PyTorch Tensors\n",
    "\n",
    "PyTorch models work with PyTorch tensors, not NumPy arrays directly. So, we convert our NumPy arrays `x_numpy` and `y_numpy` into tensors.\n",
    "\n",
    "-   `torch.from_numpy()`: This function does the conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ccc6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "x_tensor = torch.from_numpy(x_numpy)\n",
    "y_tensor = torch.from_numpy(y_numpy)\n",
    "\n",
    "print(f\"Shape of x_tensor: {x_tensor.shape}\")\n",
    "print(f\"Shape of y_tensor: {y_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168a710f",
   "metadata": {},
   "source": [
    "### 1.4 Defining the Linear Regression Model\n",
    "\n",
    "We define our model as a Python class that inherits from `nn.Module`. This is the standard way to create models in PyTorch.\n",
    "\n",
    "-   `__init__(self)`: The constructor. Here, we define the layers of our model. \n",
    "    -   `super(LinearRegression, self).__init__()`: Calls the constructor of the parent class (`nn.Module`). This is necessary.\n",
    "    -   `self.linear = nn.Linear(1, 1)`: This creates a linear layer. \n",
    "        -   The first `1` means the input to this layer has 1 feature (our `x` value).\n",
    "        -   The second `1` means the output of this layer has 1 feature (our predicted `y` value).\n",
    "        -   This layer will learn a weight (slope) and a bias (intercept) for the equation `y = weight * x + bias`.\n",
    "-   `forward(self, x)`: This method defines how the input `x` flows through the model to produce an output. Here, it simply passes `x` through our linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dcaa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple linear regression model\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        # Input features: 1 (x), Output features: 1 (y_predicted)\n",
    "        self.linear = nn.Linear(in_features=1, out_features=1)\n",
    "      \n",
    "    def forward(self, x):\n",
    "        # This defines the computation performed at every call.\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b0d561",
   "metadata": {},
   "source": [
    "### 1.5 Initializing Model, Loss Function, and Optimizer\n",
    "\n",
    "-   **Model**: `model = LinearRegressionModel()`: We create an instance of our model class.\n",
    "-   **Loss Function**: `criterion = nn.MSELoss()`: The loss function measures how different our model's predictions are from the actual `y` values. \n",
    "    -   `MSELoss` stands for Mean Squared Error. It calculates the average of the squares of the differences between predictions and actual values. This is a common loss function for regression tasks.\n",
    "-   **Optimizer**: `optimizer = torch.optim.SGD(model.parameters(), lr=0.01)`: The optimizer is responsible for updating the model's parameters (weights and biases) to minimize the loss.\n",
    "    -   `torch.optim.SGD`: Stochastic Gradient Descent. A common optimization algorithm.\n",
    "    -   `model.parameters()`: Tells the optimizer which parameters (weights and biases of our `self.linear` layer) it needs to update.\n",
    "    -   `lr=0.01`: Learning rate. This controls how big the steps are when updating parameters. A smaller learning rate means smaller steps and potentially longer training, but can be more stable. A larger learning rate can speed up training but might overshoot the optimal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175131e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "model = LinearRegressionModel()\n",
    "criterion = nn.MSELoss() # Mean Squared Error Loss\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) # Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447f3def",
   "metadata": {},
   "source": [
    "### 1.6 Training Loop\n",
    "\n",
    "This is where the model learns. We repeat the process for a certain number of `epochs`. An epoch is one complete pass through the entire training dataset.\n",
    "\n",
    "Inside the loop:\n",
    "1.  `y_pred = model(x_tensor)`: Forward pass. Get predictions from the model using the current `x_tensor`.\n",
    "2.  `loss = criterion(y_pred, y_tensor)`: Calculate the loss by comparing the predictions (`y_pred`) with the actual values (`y_tensor`).\n",
    "3.  `optimizer.zero_grad()`: Clear old gradients. Gradients are values that indicate the direction to adjust parameters to reduce loss. We need to clear them before calculating new ones for the current step, otherwise they accumulate.\n",
    "4.  `loss.backward()`: Backward pass. Calculate the gradients of the loss with respect to the model's parameters. This is where PyTorch's automatic differentiation (autograd) shines.\n",
    "5.  `optimizer.step()`: Update parameters. The optimizer adjusts the model's parameters using the calculated gradients and the learning rate.\n",
    "6.  `if epoch % 20 == 0:`: Print the loss every 20 epochs to monitor training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5954f698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "epochs = 100\n",
    "losses = []  # Initialize list to store loss values for plotting\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 1. Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x_tensor)\n",
    "    \n",
    "    # 2. Compute loss\n",
    "    loss = criterion(y_pred, y_tensor)\n",
    "    losses.append(loss.item()) # .item() gets the scalar value of the loss\n",
    "    \n",
    "    # 3. Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()  # Clears existing gradients\n",
    "    loss.backward()      # Computes gradients of the loss w.r.t. parameters\n",
    "    optimizer.step()       # Updates parameters based on gradients\n",
    "      \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Print Learned Parameters and Visualize Results\n",
    "\n",
    "After training, our model's `linear` layer has learned a weight (slope) and a bias (intercept). We expect these to be close to 2 and 1, respectively, since our original data was generated from `y = 2x + 1`.\n",
    "\n",
    "-   `model.linear.weight.item()`: Gets the learned weight (slope).\n",
    "-   `model.linear.bias.item()`: Gets the learned bias (intercept).\n",
    "\n",
    "We also plot the original data, the true line, and the line our model learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print learned parameters\n",
    "learned_weight = model.linear.weight.item()\n",
    "learned_bias = model.linear.bias.item()\n",
    "print(f\"\\nLearned equation: y = {learned_weight:.4f}x + {learned_bias:.4f}\")\n",
    "\n",
    "# Plot the training loss\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(range(epochs), losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Visualize the learned line\n",
    "model.eval() # Set the model to evaluation mode (important for some layers like dropout, batchnorm)\n",
    "with torch.no_grad(): # Disable gradient calculation for inference\n",
    "    predicted_line = model(x_tensor).detach().numpy() # Get predictions and convert to numpy\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x_numpy, y_numpy, label='Original data', color='blue', s=10)\n",
    "plt.plot(x_numpy, 2 * x_numpy + 1, label='True line (y=2x+1)', color='red', linestyle='--')\n",
    "plt.plot(x_numpy, predicted_line, label='Learned line', color='green')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear Regression Result')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 PyTorch Practice Variation: Different Optimizer and Loss Function\n",
    "\n",
    "The original example uses Stochastic Gradient Descent (`SGD`) as the optimizer and Mean Squared Error (`MSELoss`) as the loss function. Let's try changing these:\n",
    "\n",
    "-   **Optimizer**: We can use `Adam` (Adaptive Moment Estimation), which is often a good default optimizer and can converge faster or achieve better results than SGD for some problems. It adapts the learning rate for each parameter.\n",
    "-   **Loss Function**: We can use `L1Loss` (Mean Absolute Error or MAE). Instead of squaring the differences between predicted and actual values, `L1Loss` takes the absolute difference. This can make the model less sensitive to outliers compared to `MSELoss`.\n",
    "\n",
    "We'll repeat the process with these changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize model (to reset learned parameters)\n",
    "model_variation = LinearRegressionModel()\n",
    "\n",
    "# Variation: Use Adam optimizer and L1Loss\n",
    "criterion_variation = nn.L1Loss()  # Mean Absolute Error\n",
    "optimizer_variation = torch.optim.Adam(model_variation.parameters(), lr=0.01) # Adam optimizer\n",
    "\n",
    "print(\"Training with Adam optimizer and L1Loss:\\n\")\n",
    "losses_variation = []\n",
    "\n",
    "# Training loop for the variation\n",
    "for epoch in range(epochs): # Using the same number of epochs\n",
    "    y_pred_variation = model_variation(x_tensor)\n",
    "    loss_variation = criterion_variation(y_pred_variation, y_tensor)\n",
    "    losses_variation.append(loss_variation.item())\n",
    "    \n",
    "    optimizer_variation.zero_grad()\n",
    "    loss_variation.backward()\n",
    "    optimizer_variation.step()\n",
    "      \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss (L1): {loss_variation.item():.4f}\")\n",
    "\n",
    "# Print learned parameters for the variation\n",
    "learned_weight_var = model_variation.linear.weight.item()\n",
    "learned_bias_var = model_variation.linear.bias.item()\n",
    "print(f\"\\nLearned equation (variation): y = {learned_weight_var:.4f}x + {learned_bias_var:.4f}\")\n",
    "\n",
    "# Plot the training loss for the variation\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(range(epochs), losses_variation, label='L1 Loss with Adam')\n",
    "plt.plot(range(epochs), losses, label='MSE Loss with SGD (original)', linestyle='--') # Plot original for comparison\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Comparison: Variation vs Original')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Visualize the learned line for the variation\n",
    "model_variation.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_line_var = model_variation(x_tensor).detach().numpy()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x_numpy, y_numpy, label='Original data', color='blue', s=10)\n",
    "plt.plot(x_numpy, 2 * x_numpy + 1, label='True line (y=2x+1)', color='red', linestyle='--')\n",
    "plt.plot(x_numpy, predicted_line_var, label='Learned line (Adam, L1Loss)', color='purple')\n",
    "plt.plot(x_numpy, predicted_line, label='Learned line (SGD, MSELoss) - original', color='green', linestyle=':')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear Regression Result: Variation vs Original')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can observe how the choice of optimizer and loss function might affect the training process (e.g., speed of convergence, final loss value) and the resulting learned parameters. Adam often converges faster than basic SGD."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (classification_models_py3.12)",
   "language": "python",
   "name": "classification_models_py3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
