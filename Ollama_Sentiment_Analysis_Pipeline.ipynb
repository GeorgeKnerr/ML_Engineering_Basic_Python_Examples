{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama Sentiment Analysis Pipeline — Tutorial\n",
    "\n",
    "Welcome to this hands-on tutorial on building a **sentiment analysis pipeline** using **Ollama**, an open-source platform for running Large Language Models (LLMs) locally or on-premise.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "In this tutorial, you will:\n",
    "- 🔧 Connect to an Ollama API endpoint and verify connectivity\n",
    "- 📊 Load and preprocess text data from CSV files\n",
    "- 🤖 Use LLMs for intelligent column detection and text analysis\n",
    "- 🎭 Perform sentiment classification and emotion detection\n",
    "- ✅ Validate and structure LLM outputs using Pydantic models\n",
    "- 💾 Save results for downstream analysis\n",
    "\n",
    "## Why Ollama?\n",
    "\n",
    "Unlike cloud-based solutions (Azure OpenAI, AWS Bedrock, etc.), Ollama allows you to:\n",
    "- Run models **entirely on-premise** (no data leaves your network)\n",
    "- Avoid per-token API costs\n",
    "- Maintain full control over your infrastructure\n",
    "- Use open-source models (Llama, Mistral, Gemma, etc.)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, ensure you have:\n",
    "1. **Ollama installed and running** (download from [ollama.ai](https://ollama.ai))\n",
    "2. A model pulled (e.g., `ollama pull gemma3` or `ollama pull llama3`)\n",
    "3. Python 3.8+ with required packages installed\n",
    "4. A dataset with text reviews (we provide a sample mental health dataset)\n",
    "     - URL to the dataset provided in this repo (./datasets/Mental Health Dataset.csv.zip)  [Licnese](https://creativecommons.org/licenses/by/4.0/)\n",
    "        - https://www.kaggle.com/datasets/sujaykapadnis/mental-health-insights-data\n",
    "        - Be sure to extract the dataset file and update the DATA_SET_PATH variable.\n",
    "\n",
    "👉 **No authentication or API keys required!**\n",
    "\n",
    "Let's get started! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Setup and Imports\n",
    "\n",
    "First, we'll import all the Python libraries needed for this tutorial. Understanding what each library does will help you customize the pipeline later.\n",
    "\n",
    "### Libraries We're Using:\n",
    "\n",
    "| Library | Purpose |\n",
    "|---------|---------|\n",
    "| `pandas` | Data manipulation and CSV handling |\n",
    "| `requests` | HTTP communication with Ollama API |\n",
    "| `json` | Parsing JSON responses from the model |\n",
    "| `difflib` | Fuzzy string matching for column detection |\n",
    "| `re` | Regular expressions for cleaning LLM output |\n",
    "| `tenacity` | Retry logic for network resilience |\n",
    "| `pydantic` | Data validation and schema enforcement |\n",
    "| `typing` | Type hints for better code clarity |\n",
    "\n",
    "### Why Import Everything Up Front?\n",
    "\n",
    "Following Python best practices, we consolidate all imports at the top of the notebook. This makes it easy to:\n",
    "- See all dependencies at a glance\n",
    "- Identify missing packages before running the pipeline\n",
    "- Maintain clean, organized code\n",
    "\n",
    "Run this cell first to ensure all dependencies are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a657a776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import textwrap\n",
    "import requests\n",
    "import json\n",
    "import difflib\n",
    "import re\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347c46cb",
   "metadata": {},
   "source": [
    "## 2) Configure Ollama Connection\n",
    "\n",
    "Now we'll configure the connection parameters for your Ollama instance. This is the only section you'll need to customize for your environment.\n",
    "\n",
    "### Configuration Parameters:\n",
    "\n",
    "**`OLLAMA_URL`** - The HTTP endpoint where Ollama is listening\n",
    "- Default: `http://localhost:11434/api/chat` (Ollama's default local server)\n",
    "- Change this if Ollama runs on a different machine or port\n",
    "- Example: `http://192.168.1.100:11434/api/chat` for a server on your network\n",
    "\n",
    "**`MODEL_NAME`** - The LLM model to use for analysis\n",
    "- We're using `gemma3` (Google's efficient model, good for sentiment tasks)\n",
    "- Other recommended options:\n",
    "  - `llama3` - Meta's powerful general-purpose model\n",
    "  - `mistral` - Fast and accurate for text classification\n",
    "  - `phi3` - Microsoft's small but capable model\n",
    "  - `qwen2` - Strong multilingual support\n",
    "\n",
    "**`DATA_SET_PATH`** - Location of your input CSV file\n",
    "- Default: `./datasets/mental-health-dataset.csv`\n",
    "- Can be an absolute path or relative to this notebook\n",
    "\n",
    "### Column Detection Settings:\n",
    "\n",
    "**`FORCE_OLLAMA_COLUMN_SELECTION`** - Always use LLM for column detection\n",
    "- `False` (default): Use fast heuristic first, LLM only if ambiguous\n",
    "- `True`: Always ask the LLM (slower but more accurate)\n",
    "\n",
    "**`ENABLE_OLLAMA_FALLBACK`** - Use LLM when heuristic is unsure\n",
    "- `True` (default): Get LLM help when needed\n",
    "- `False`: Fall back to first text column if heuristic fails\n",
    "\n",
    "💡 **Tip**: Start with defaults, then adjust based on your dataset characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcd5523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama configuration\n",
    "OLLAMA_URL = \"http://localhost:11434/api/chat\"  # 🔧 Change this to your actual Ollama endpoint\n",
    "MODEL_NAME = \"gemma3\"  # Choose a good model for sentiment tasks\n",
    "DATA_SET_PATH = \"./datasets/mental-health-dataset.csv\"  # Path to your dataset\n",
    "\n",
    "# Column detection settings\n",
    "FORCE_OLLAMA_COLUMN_SELECTION = True  # Set True to always use LLM for column detection (slower but smarter)\n",
    "ENABLE_OLLAMA_FALLBACK = True  # Set True to use LLM when heuristic is ambiguous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97faeeae",
   "metadata": {},
   "source": [
    "## 3) Define Helper Functions\n",
    "\n",
    "Before diving into the analysis, we'll define two reusable helper functions that encapsulate common operations. This follows the **DRY principle** (Don't Repeat Yourself) and makes the code more maintainable.\n",
    "\n",
    "### Function 1: `query_ollama()`\n",
    "\n",
    "This is the **core communication function** that sends prompts to Ollama and handles streaming responses.\n",
    "\n",
    "**Key Features:**\n",
    "- ✅ **Streaming support** - Processes NDJSON responses incrementally\n",
    "- ✅ **Retry logic** - Automatically retries on network failures (via `@retry` decorator)\n",
    "- ✅ **Error handling** - Gracefully handles malformed responses\n",
    "- ✅ **Verbose mode** - Optional diagnostic output for troubleshooting\n",
    "\n",
    "**How it works:**\n",
    "1. Constructs a chat-format payload (compatible with OpenAI API format)\n",
    "2. Sends POST request to Ollama with streaming enabled\n",
    "3. Iterates through NDJSON lines, extracting message content\n",
    "4. Returns assembled response text\n",
    "\n",
    "**Why streaming?** Ollama sends responses incrementally (token by token), which allows for:\n",
    "- Real-time feedback in production applications\n",
    "- Lower memory usage for long responses\n",
    "- Ability to stop generation early if needed\n",
    "\n",
    "### Function 2: `extract_json()`\n",
    "\n",
    "LLMs sometimes wrap JSON in markdown code fences (` ```json ... ``` `), which breaks JSON parsers. This function cleans the response.\n",
    "\n",
    "**Key Features:**\n",
    "- ✅ Removes markdown code fences (` ``` `, ` ```json `)\n",
    "- ✅ Strips whitespace\n",
    "- ✅ Safe for responses that are already clean JSON\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# Input:  \"```json\\n{\\\"key\\\": \\\"value\\\"}\\n```\"\n",
    "# Output: \"{\\\"key\\\": \\\"value\\\"}\"\n",
    "```\n",
    "\n",
    "These functions will be used throughout the notebook, making later cells much cleaner and easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d1cb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=10, max=30), stop=stop_after_attempt(3))\n",
    "def query_ollama(prompt, model=MODEL_NAME, verbose=False):\n",
    "    \"\"\"\n",
    "    Send a prompt to Ollama and return the assembled response.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The text prompt to send\n",
    "        model: Model name to use (default: MODEL_NAME)\n",
    "        verbose: If True, print diagnostic info during processing\n",
    "    \n",
    "    Returns:\n",
    "        Assembled response text from the model\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Sending request to {OLLAMA_URL} with model {model}...')\n",
    "    \n",
    "    response = requests.post(OLLAMA_URL, json=payload, timeout=60, stream=True)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Handle streaming NDJSON response\n",
    "    assembled = ''\n",
    "    for line in response.iter_lines(decode_unicode=False):\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            obj = json.loads(line.decode('utf-8', errors='replace'))\n",
    "            if isinstance(obj, dict):\n",
    "                msg = obj.get('message')\n",
    "                if isinstance(msg, dict) and 'content' in msg:\n",
    "                    assembled += str(msg['content'])\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    return assembled if assembled else \"No response content\"\n",
    "\n",
    "\n",
    "def extract_json(text):\n",
    "    \"\"\"\n",
    "    Extract JSON from response, handling markdown code fences.\n",
    "    \n",
    "    Args:\n",
    "        text: Response text that may contain JSON wrapped in ```json...```\n",
    "    \n",
    "    Returns:\n",
    "        Cleaned JSON string\n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "    # Remove markdown code fences if present\n",
    "    if text.startswith('```'):\n",
    "        text = re.sub(r'^```(?:json)?\\s*\\n?', '', text)\n",
    "        text = re.sub(r'\\n?```\\s*$', '', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d3054f",
   "metadata": {},
   "source": [
    "## 4) Quick Ollama Connectivity Test\n",
    "\n",
    "Before processing your data, it's crucial to verify that Ollama is reachable and responding correctly. This diagnostic step will save you time troubleshooting later.\n",
    "\n",
    "### What This Test Does:\n",
    "\n",
    "1. **Displays configuration** - Shows the URL and model you're using\n",
    "2. **Sends a simple prompt** - Asks the model to reply with \"PONG\"\n",
    "3. **Validates the response** - Checks if the model understood the request\n",
    "4. **Reports status** - Clear success/failure messages with troubleshooting hints\n",
    "\n",
    "### Possible Outcomes:\n",
    "\n",
    "✅ **Success** - Ollama is working, proceed with confidence!\n",
    "\n",
    "⚠️ **Unexpected Response** - Model responded but didn't say \"PONG\"\n",
    "   - May indicate prompt understanding issues\n",
    "   - Check if the correct model is loaded\n",
    "\n",
    "❌ **Connection Error** - Can't reach Ollama\n",
    "   - Verify Ollama is running: `ollama serve` (in terminal)\n",
    "   - Check `OLLAMA_URL` is correct\n",
    "   - Ensure no firewall is blocking port 11434\n",
    "\n",
    "❌ **Timeout** - Request took too long\n",
    "   - Model might be loading (first run can be slow)\n",
    "   - Check system resources (CPU/RAM/GPU usage)\n",
    "   - Try a smaller model\n",
    "\n",
    "### Troubleshooting Tips:\n",
    "\n",
    "- **Not installed?** Run `curl -fsSL https://ollama.com/install.sh | sh` (Linux/Mac)\n",
    "- **Model not pulled?** Run `ollama pull gemma3` in your terminal\n",
    "- **Port conflict?** Check if another service is using port 11434\n",
    "\n",
    "Run this cell now - don't skip it! A working connection is essential for everything that follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9431b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('OLLAMA_URL:', OLLAMA_URL)\n",
    "print('MODEL_NAME:', MODEL_NAME)\n",
    "print('\\n-- Testing Ollama connectivity --')\n",
    "\n",
    "try:\n",
    "    test_response = query_ollama(\"Please reply with the single token: PONG\", verbose=True)\n",
    "    print(f'\\nResponse: {test_response}')\n",
    "    \n",
    "    if 'PONG' in test_response.upper():\n",
    "        print('\\n✅ SUCCESS: Ollama is reachable and responding correctly!')\n",
    "    else:\n",
    "        print(f'\\n⚠️ WARNING: Got unexpected response: {test_response[:200]}')\n",
    "        \n",
    "except requests.exceptions.ConnectionError as ce:\n",
    "    print(f'❌ Connection error: {ce}')\n",
    "    print('Please verify OLLAMA_URL is correct and Ollama is running.')\n",
    "except requests.exceptions.Timeout as te:\n",
    "    print(f'❌ Request timed out: {te}')\n",
    "except Exception as e:\n",
    "    print(f'❌ Unexpected error: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cb71a0",
   "metadata": {},
   "source": [
    "## 5) Load Dataset\n",
    "\n",
    "Now we'll load your CSV dataset using pandas. This tutorial uses a **mental health dataset** with patient reviews, but the pipeline works with any text data.\n",
    "\n",
    "### What Happens Here:\n",
    "\n",
    "The code performs two operations:\n",
    "1. **Reads the CSV file** specified in `DATA_SET_PATH`\n",
    "2. **Displays the first 5 rows** using `.head()` for quick inspection\n",
    "\n",
    "### Understanding Your Data:\n",
    "\n",
    "After running this cell, examine the output to answer:\n",
    "- ✅ How many columns does the dataset have?\n",
    "- ✅ What are the column names?\n",
    "- ✅ Which column likely contains the review text?\n",
    "- ✅ Are there any missing values (NaN)?\n",
    "- ✅ What's the data type of each column?\n",
    "\n",
    "### Common Dataset Structures:\n",
    "\n",
    "Your CSV might look like one of these patterns:\n",
    "\n",
    "**Pattern 1: Simple reviews**\n",
    "```\n",
    "review_text, rating\n",
    "\"Great product!\", 5\n",
    "\"Not satisfied\", 2\n",
    "```\n",
    "\n",
    "**Pattern 2: Social media posts**\n",
    "```\n",
    "post_id, username, content, timestamp, likes\n",
    "123, user1, \"Loving this!\", 2024-01-01, 42\n",
    "```\n",
    "\n",
    "**Pattern 3: Survey responses**\n",
    "```\n",
    "respondent_id, question_1, question_2, comments\n",
    "A001, 4, 5, \"Additional feedback here\"\n",
    "```\n",
    "\n",
    "The next step (column detection) will intelligently identify which column contains your text data, regardless of the structure.\n",
    "\n",
    "### Troubleshooting:\n",
    "\n",
    "- **File not found?** Check `DATA_SET_PATH` points to the correct location\n",
    "- **Encoding errors?** Try adding `encoding='utf-8'` or `encoding='latin-1'` to `pd.read_csv()`\n",
    "- **Large file?** Use `pd.read_csv(path, nrows=1000)` to load only the first 1000 rows for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97c3497",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(DATA_SET_PATH)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e9a142",
   "metadata": {},
   "source": [
    "## 6) Inspect a Single Review — Robust Column Detection\n",
    "\n",
    "This cell automatically detects which column contains the review text using either:\n",
    "1. **LLM-based detection** (if `FORCE_OLLAMA_COLUMN_SELECTION=True`) - slower but more accurate\n",
    "2. **Heuristic matching** - fast keyword and fuzzy matching\n",
    "3. **LLM fallback** (if `ENABLE_OLLAMA_FALLBACK=True`) - used when heuristic is ambiguous\n",
    "\n",
    "The cell then displays the first review from the selected column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0c1ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Available columns:', list(reviews.columns))\n",
    "\n",
    "target = 'review_text'\n",
    "chosen_col = None\n",
    "\n",
    "def call_ollama_for_column(cols):\n",
    "    \"\"\"Ask Ollama to identify the text column from the list.\"\"\"\n",
    "    prompt = (\n",
    "        \"You are a helpful assistant for a data-preprocessing script.\\n\"\n",
    "        \"Given the following dataset columns: \\n\"\n",
    "        f\"{cols}\\n\\n\"\n",
    "        \"Which column is most likely to contain the main user-generated review, comment, or post text?\\n\"\n",
    "        \"Answer with the exact column name only, nothing else.\"\n",
    "    )\n",
    "    try:\n",
    "        reply = query_ollama(prompt).strip().splitlines()[0].strip().strip('\"').strip(\"'\")\n",
    "        print(f\"Ollama replied: {reply}\")\n",
    "        \n",
    "        # Try exact match\n",
    "        if reply in reviews.columns:\n",
    "            return reply\n",
    "        # Try case-insensitive match\n",
    "        ci = [c for c in reviews.columns if c.lower() == reply.lower()]\n",
    "        if ci:\n",
    "            return ci[0]\n",
    "        # Try fuzzy match\n",
    "        close = difflib.get_close_matches(reply, reviews.columns, n=1, cutoff=0.6)\n",
    "        if close:\n",
    "            print(f\"Using fuzzy match to column: {close[0]}\")\n",
    "            return close[0]\n",
    "        print('Ollama reply did not match any column.')\n",
    "    except Exception as e:\n",
    "        print(f\"Ollama call failed: {e}\")\n",
    "    return None\n",
    "\n",
    "# Strategy 1: Force LLM selection\n",
    "if FORCE_OLLAMA_COLUMN_SELECTION:\n",
    "    print('Using LLM for column selection...\\n')\n",
    "    chosen_col = call_ollama_for_column(list(reviews.columns))\n",
    "    if chosen_col:\n",
    "        print(f\"✅ LLM selected column: {chosen_col}\")\n",
    "\n",
    "# Strategy 2: Exact match or heuristic\n",
    "if chosen_col is None:\n",
    "    if target in reviews.columns:\n",
    "        chosen_col = target\n",
    "        print(f\"✅ Found exact match: {chosen_col}\")\n",
    "    else:\n",
    "        # Heuristic scoring\n",
    "        synonyms = ['review', 'text', 'comment', 'body', 'post', 'content', 'message', 'feedback']\n",
    "        scores = {}\n",
    "        for col in reviews.columns:\n",
    "            col_low = col.lower()\n",
    "            score = sum(2.0 for syn in synonyms if syn in col_low)\n",
    "            score += difflib.SequenceMatcher(None, col_low, target).ratio()\n",
    "            scores[col] = score\n",
    "        \n",
    "        best_col = max(scores, key=scores.get)\n",
    "        best_score = scores[best_col]\n",
    "        \n",
    "        if best_score > 0.3:\n",
    "            chosen_col = best_col\n",
    "            print(f\"✅ Heuristic selected: {best_col} (score={best_score:.2f})\")\n",
    "        else:\n",
    "            print(f\"⚠️ Heuristic ambiguous (best={best_col}, score={best_score:.2f})\")\n",
    "            \n",
    "            # Strategy 3: LLM fallback\n",
    "            if ENABLE_OLLAMA_FALLBACK:\n",
    "                print('Falling back to LLM for column selection...')\n",
    "                ol_col = call_ollama_for_column(list(reviews.columns))\n",
    "                if ol_col:\n",
    "                    chosen_col = ol_col\n",
    "                    print(f\"✅ LLM fallback selected: {chosen_col}\")\n",
    "            \n",
    "            # Strategy 4: Dtype-based fallback\n",
    "            if chosen_col is None:\n",
    "                obj_cols = reviews.select_dtypes(include=['object']).columns.tolist()\n",
    "                if obj_cols:\n",
    "                    chosen_col = obj_cols[0]\n",
    "                    print(f\"⚠️ Falling back to first object column: {chosen_col}\")\n",
    "                else:\n",
    "                    raise KeyError(f\"No text column found. Available: {list(reviews.columns)}\")\n",
    "\n",
    "# Display the first review\n",
    "review = reviews[chosen_col].iloc[0]\n",
    "print(f\"\\n--- First value from '{chosen_col}' ---\\n\")\n",
    "review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4741b578",
   "metadata": {},
   "source": [
    "## 7) Create Sentiment Analysis Prompt\n",
    "\n",
    "Prompts are the \"instructions\" we give to LLMs. Good prompts = good results! Let's break down what makes this prompt effective.\n",
    "\n",
    "### The Prompt Template:\n",
    "\n",
    "```\n",
    "You are a sentiment analysis expert. Analyze the following patient review and return a JSON object with:\n",
    "- sentiment_label: 'positive', 'negative', or 'neutral'\n",
    "- confidence_score: a number between 0 and 1\n",
    "- emotions: a list of detected emotions like ['joy', 'anger', 'sadness', 'fear', 'surprise']\n",
    "Return only the JSON object and no extra text.\n",
    "\n",
    "Review text:\n",
    "{text}\n",
    "```\n",
    "\n",
    "### Prompt Engineering Principles Applied:\n",
    "\n",
    "#### 1. **Role Assignment** (\"You are a sentiment analysis expert\")\n",
    "- Sets the model's context and expertise domain\n",
    "- Encourages specialized, high-quality responses\n",
    "- Similar to how you'd brief a human analyst\n",
    "\n",
    "#### 2. **Clear Task Definition** (\"Analyze the following patient review\")\n",
    "- Specifies exactly what needs to be done\n",
    "- Provides domain context (\"patient review\" vs generic text)\n",
    "- Helps model understand the expected tone and content\n",
    "\n",
    "#### 3. **Structured Output Requirements**\n",
    "- Lists exact fields needed: `sentiment_label`, `confidence_score`, `emotions`\n",
    "- Specifies data types and formats\n",
    "- Provides examples for complex fields (emotion list)\n",
    "\n",
    "#### 4. **Constraints** (\"Return only the JSON object and no extra text\")\n",
    "- Critical for parsing! Prevents preambles like \"Here's the analysis:\"\n",
    "- Ensures machine-readable output\n",
    "- Reduces the need for complex text cleanup\n",
    "\n",
    "#### 5. **Format Specification** (\"JSON object\")\n",
    "- Ensures consistent, parseable responses\n",
    "- Enables automated validation with Pydantic\n",
    "- Makes batch processing reliable\n",
    "\n",
    "### Why Three Sentiment Labels?\n",
    "\n",
    "We use `positive`, `negative`, and `neutral` (not just positive/negative) because:\n",
    "- ✅ Many reviews are mixed or balanced\n",
    "- ✅ Neutral sentiment is valuable information\n",
    "- ✅ Reflects real-world sentiment distribution more accurately\n",
    "\n",
    "### Confidence Scores:\n",
    "\n",
    "The `confidence_score` (0.0 to 1.0) helps you:\n",
    "- Filter low-confidence predictions for human review\n",
    "- Weight results in aggregate statistics\n",
    "- Identify edge cases where sentiment is unclear\n",
    "\n",
    "### Emotion Detection:\n",
    "\n",
    "Beyond binary sentiment, emotions provide **richer insights**:\n",
    "- `positive` + `joy` = satisfaction\n",
    "- `positive` + `surprise` = delight/exceeded expectations\n",
    "- `negative` + `fear` = anxiety/concern\n",
    "- `negative` + `anger` = frustration/complaint\n",
    "\n",
    "### Customizing the Prompt:\n",
    "\n",
    "You can modify this template to fit your needs:\n",
    "\n",
    "**For product reviews:**\n",
    "```python\n",
    "PROMPT_TEMPLATE = '''Analyze this product review and return JSON with:\n",
    "- sentiment_label: 'positive', 'negative', or 'neutral'\n",
    "- confidence_score: 0 to 1\n",
    "- aspects: list of product features mentioned (e.g., ['quality', 'price', 'shipping'])\n",
    "- recommendation: would the reviewer recommend this? (true/false)\n",
    "'''\n",
    "```\n",
    "\n",
    "**For customer support tickets:**\n",
    "```python\n",
    "PROMPT_TEMPLATE = '''Analyze this support ticket and return JSON with:\n",
    "- sentiment_label: 'positive', 'negative', or 'neutral'\n",
    "- urgency: 'low', 'medium', or 'high'\n",
    "- category: type of issue (e.g., 'technical', 'billing', 'feature request')\n",
    "- requires_human: boolean (true if complex/sensitive)\n",
    "'''\n",
    "```\n",
    "\n",
    "**For social media posts:**\n",
    "```python\n",
    "PROMPT_TEMPLATE = '''Analyze this social media post and return JSON:\n",
    "- sentiment_label: 'positive', 'negative', or 'neutral'\n",
    "- intent: 'complaint', 'praise', 'question', or 'general'\n",
    "- brand_mentioned: boolean\n",
    "- engagement_potential: 'low', 'medium', or 'high'\n",
    "'''\n",
    "```\n",
    "\n",
    "### Testing Your Prompt:\n",
    "\n",
    "Before running on your full dataset:\n",
    "1. Test with 3-5 diverse examples\n",
    "2. Check if JSON is valid and complete\n",
    "3. Verify sentiment labels make sense\n",
    "4. Ensure confidence scores are reasonable (not all 0.5 or all 1.0)\n",
    "\n",
    "The next cell will test this prompt on a single review - perfect for validation! 🔍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a07187",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = '''You are a sentiment analysis expert. Analyze the following patient review and return a JSON object with:\n",
    "- sentiment_label: 'positive', 'negative', or 'neutral'\n",
    "- confidence_score: a number between 0 and 1\n",
    "- emotions: a list of detected emotions like ['joy', 'anger', 'sadness', 'fear', 'surprise']\n",
    "Return only the JSON object and no extra text.\n",
    "\n",
    "Review text:\n",
    "{text}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0c12b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PROMPT_TEMPLATE.format(text=review)\n",
    "response_text = query_ollama(prompt)\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12ad0f7",
   "metadata": {},
   "source": [
    "## 9) Batch Sentiment Analysis\n",
    "Apply the same query to multiple reviews and store the structured results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5c9a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for text in reviews[chosen_col].head(3):  # Use detected column, analyze first 3 for demo\n",
    "    prompt = PROMPT_TEMPLATE.format(text=text)\n",
    "    output = query_ollama(prompt)\n",
    "    results.append(output)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32888a20",
   "metadata": {},
   "source": [
    "## 10) Parse and Validate Results\n",
    "Convert each model output to JSON, then use **Pydantic** to ensure valid fields and types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a254771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentOutput(BaseModel):\n",
    "    sentiment_label: str = Field(..., description=\"'positive'|'negative'|'neutral'\")\n",
    "    confidence_score: float = Field(..., description=\"Value between 0 and 1\")\n",
    "    emotions: List[str]\n",
    "\n",
    "parsed = []\n",
    "for res in results:\n",
    "    try:\n",
    "        cleaned = extract_json(res)\n",
    "        data = json.loads(cleaned)\n",
    "        parsed.append(SentimentOutput(**data).model_dump())\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not parse response: {e}\\nResponse was: {res[:200]}...\")\n",
    "\n",
    "df_results = pd.DataFrame(parsed)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11) Save Results\n",
    "\n",
    "You've done the hard work of analysis - now let's preserve it! This cell saves your validated results to a CSV file for future use.\n",
    "\n",
    "### What This Cell Does:\n",
    "\n",
    "```python\n",
    "df_results.to_csv('ollama_sentiment_results.csv', index=False)\n",
    "```\n",
    "\n",
    "**Parameters explained:**\n",
    "- **Filename**: `'ollama_sentiment_results.csv'` - Saved in the same directory as this notebook\n",
    "- **`index=False`**: Excludes row numbers (0, 1, 2...) from the CSV - usually what you want\n",
    "\n",
    "### The Output File:\n",
    "\n",
    "Opens in Excel, Google Sheets, or any CSV viewer:\n",
    "\n",
    "```csv\n",
    "sentiment_label,confidence_score,emotions\n",
    "negative,0.85,\"['fear', 'sadness', 'anger']\"\n",
    "positive,0.92,\"['joy']\"\n",
    "neutral,0.71,\"['surprise']\"\n",
    "```\n",
    "\n",
    "### Why Save to CSV?\n",
    "\n",
    "✅ **Universal format** - Readable by virtually every data tool  \n",
    "✅ **Version control friendly** - Git can track changes  \n",
    "✅ **Portable** - Easy to share with colleagues  \n",
    "✅ **No dependencies** - Doesn't require Python to open  \n",
    "✅ **Merge-able** - Combine with other datasets easily\n",
    "\n",
    "### Alternative Storage Options:\n",
    "\n",
    "#### 1. **Excel format (for non-technical stakeholders)**\n",
    "```python\n",
    "df_results.to_excel('sentiment_results.xlsx', index=False, engine='openpyxl')\n",
    "```\n",
    "\n",
    "#### 2. **JSON (for APIs or JavaScript applications)**\n",
    "```python\n",
    "df_results.to_json('sentiment_results.json', orient='records', indent=2)\n",
    "```\n",
    "\n",
    "#### 3. **Parquet (for big data pipelines)**\n",
    "```python\n",
    "df_results.to_parquet('sentiment_results.parquet', index=False)\n",
    "```\n",
    "\n",
    "#### 4. **Database (for production systems)**\n",
    "```python\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine('postgresql://user:password@localhost/dbname')\n",
    "df_results.to_sql('sentiment_analysis', engine, if_exists='append', index=False)\n",
    "```\n",
    "\n",
    "#### 5. **Pickle (preserves exact Python objects)**\n",
    "```python\n",
    "import pickle\n",
    "with open('sentiment_results.pkl', 'wb') as f:\n",
    "    pickle.dump(df_results, f)\n",
    "```\n",
    "\n",
    "### File Naming Best Practices:\n",
    "\n",
    "Include metadata in filenames for future you:\n",
    "\n",
    "```python\n",
    "from datetime import datetime\n",
    "\n",
    "# With timestamp\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "df_results.to_csv(f'sentiment_{timestamp}.csv', index=False)\n",
    "\n",
    "# With dataset name and model\n",
    "df_results.to_csv(f'sentiment_{MODEL_NAME}_mentalhealth.csv', index=False)\n",
    "\n",
    "# With row count\n",
    "df_results.to_csv(f'sentiment_{len(df_results)}_reviews.csv', index=False)\n",
    "```\n",
    "\n",
    "### What to Do with the Results:\n",
    "\n",
    "**Immediate analysis:**\n",
    "```python\n",
    "# Sentiment distribution\n",
    "print(df_results['sentiment_label'].value_counts())\n",
    "\n",
    "# Average confidence\n",
    "print(f\"Mean confidence: {df_results['confidence_score'].mean():.2f}\")\n",
    "\n",
    "# Most common emotions\n",
    "from collections import Counter\n",
    "all_emotions = [e for sublist in df_results['emotions'] for e in sublist]\n",
    "print(Counter(all_emotions).most_common(5))\n",
    "```\n",
    "\n",
    "**Visualization:**\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_results['sentiment_label'].value_counts().plot(kind='bar')\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Merge with original data:**\n",
    "```python\n",
    "# Combine with original reviews for context\n",
    "reviews_with_sentiment = reviews.head(3).copy()\n",
    "reviews_with_sentiment = pd.concat([reviews_with_sentiment, df_results], axis=1)\n",
    "reviews_with_sentiment.to_csv('reviews_analyzed.csv', index=False)\n",
    "```\n",
    "\n",
    "### Archiving Best Practices:\n",
    "\n",
    "For production pipelines:\n",
    "1. **Version your results** - Include dates/run IDs\n",
    "2. **Keep metadata** - Save model name, parameters, timestamps\n",
    "3. **Document changes** - Maintain a changelog or README\n",
    "4. **Backup regularly** - Don't rely on a single storage location\n",
    "\n",
    "### Next Steps After Saving:\n",
    "\n",
    "✅ Share the CSV with stakeholders  \n",
    "✅ Build dashboards (Tableau, Power BI, Streamlit)  \n",
    "✅ Run statistical tests on the results  \n",
    "✅ Compare with human-labeled data (if available)  \n",
    "✅ Set up automated reporting  \n",
    "✅ Integrate into production systems\n",
    "\n",
    "Your analysis is now preserved and ready for action! 💾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Summary and Next Steps\n",
    "\n",
    "Congratulations! 🎉 You've successfully built an **end-to-end sentiment analysis pipeline** using Ollama. Let's recap what you've accomplished and explore where to go from here.\n",
    "\n",
    "---\n",
    "\n",
    "## What You've Built\n",
    "\n",
    "This pipeline demonstrates:\n",
    "\n",
    "### 1. **On-Premise LLM Integration** 🏠\n",
    "- Connected to a local Ollama instance (no cloud dependencies)\n",
    "- Handled streaming NDJSON responses efficiently\n",
    "- Implemented retry logic for network resilience\n",
    "\n",
    "### 2. **Intelligent Data Handling** 🧠\n",
    "- Automatic column detection using heuristics + LLM fallback\n",
    "- Flexible approach works across diverse dataset structures\n",
    "- Multi-strategy system balances speed and accuracy\n",
    "\n",
    "### 3. **Robust Text Analysis** 📊\n",
    "- Sentiment classification (positive/negative/neutral)\n",
    "- Confidence scoring for quality assessment\n",
    "- Emotion detection for nuanced insights\n",
    "\n",
    "### 4. **Production-Ready Engineering** ⚙️\n",
    "- Data validation using Pydantic models\n",
    "- Error handling and graceful degradation\n",
    "- Clean code structure with reusable functions\n",
    "\n",
    "### 5. **Reproducible Workflow** 🔄\n",
    "- Configuration management (easy to adapt to new environments)\n",
    "- Clear documentation for future maintainers\n",
    "- Modular design supports customization\n",
    "\n",
    "---\n",
    "\n",
    "## Key Improvements in This Version\n",
    "\n",
    "Compared to typical tutorials, this notebook features:\n",
    "\n",
    "✅ **Consolidated imports** - All dependencies declared upfront  \n",
    "✅ **Reusable helper functions** - `query_ollama` and `extract_json` used throughout  \n",
    "✅ **No code duplication** - Streaming logic centralized in one place  \n",
    "✅ **Simplified connectivity test** - From 100+ lines to 15 lines  \n",
    "✅ **Clearer configuration** - Explicit naming (`FORCE_OLLAMA_COLUMN_SELECTION` vs `FALLBACK_TO_OLLAMA`)  \n",
    "✅ **Modern Pydantic V2** - Uses `model_dump()` instead of deprecated `.dict()`  \n",
    "✅ **Comprehensive documentation** - Every cell explained in detail  \n",
    "✅ **Logical flow** - Setup → Helpers → Validation → Load → Process → Results\n",
    "\n",
    "---\n",
    "\n",
    "## Customization Ideas\n",
    "\n",
    "### Adapt to Your Use Case\n",
    "\n",
    "**For Product Reviews:**\n",
    "```python\n",
    "PROMPT_TEMPLATE = '''Analyze this product review and return JSON with:\n",
    "- sentiment_label: 'positive', 'negative', or 'neutral'\n",
    "- confidence_score: 0 to 1\n",
    "- aspects: list of product features mentioned (e.g., ['quality', 'price', 'shipping'])\n",
    "- recommendation: would the reviewer recommend this? (true/false)\n",
    "'''\n",
    "```\n",
    "\n",
    "**For Customer Support:**\n",
    "```python\n",
    "PROMPT_TEMPLATE = '''Analyze this support ticket and return JSON with:\n",
    "- sentiment_label: 'positive', 'negative', or 'neutral'\n",
    "- urgency: 'low', 'medium', or 'high'\n",
    "- category: type of issue (e.g., 'technical', 'billing', 'feature request')\n",
    "- requires_human: boolean (true if complex/sensitive)\n",
    "'''\n",
    "```\n",
    "\n",
    "**For Social Media:**\n",
    "```python\n",
    "PROMPT_TEMPLATE = '''Analyze this social media post and return JSON:\n",
    "- sentiment_label: 'positive', 'negative', or 'neutral'\n",
    "- intent: 'complaint', 'praise', 'question', or 'general'\n",
    "- brand_mentioned: boolean\n",
    "- engagement_potential: 'low', 'medium', or 'high'\n",
    "'''\n",
    "```\n",
    "\n",
    "### Performance Optimization\n",
    "\n",
    "**For Large Datasets (10k+ reviews):**\n",
    "1. Use smaller models (Phi-3, Gemma 2B)\n",
    "2. Implement parallel processing with ThreadPoolExecutor\n",
    "3. Add checkpointing to resume interrupted jobs\n",
    "4. Consider GPU acceleration if available\n",
    "\n",
    "**For Real-Time Applications:**\n",
    "1. Keep model loaded in memory (persistent Ollama service)\n",
    "2. Use async/await for concurrent requests\n",
    "3. Cache common queries\n",
    "4. Set up load balancing across multiple Ollama instances\n",
    "\n",
    "### Integration Opportunities\n",
    "\n",
    "**Database Integration:**\n",
    "```python\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('postgresql://localhost/mydb')\n",
    "df_results.to_sql('sentiment_analysis', engine, if_exists='append')\n",
    "```\n",
    "\n",
    "**API Endpoint:**\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/analyze\")\n",
    "def analyze_text(text: str):\n",
    "    prompt = PROMPT_TEMPLATE.format(text=text)\n",
    "    result = query_ollama(prompt)\n",
    "    return json.loads(extract_json(result))\n",
    "```\n",
    "\n",
    "**Scheduled Jobs:**\n",
    "```python\n",
    "from apscheduler.schedulers.blocking import BlockingScheduler\n",
    "\n",
    "scheduler = BlockingScheduler()\n",
    "\n",
    "@scheduler.scheduled_job('cron', hour=2)  # Run at 2 AM daily\n",
    "def nightly_analysis():\n",
    "    reviews = load_new_reviews()  # Your custom function\n",
    "    # Run pipeline...\n",
    "    \n",
    "scheduler.start()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Recommended Experiments\n",
    "\n",
    "### 1. **Model Comparison**\n",
    "Try different models and compare results:\n",
    "- Llama 3 8B (balanced)\n",
    "- Mistral 7B (fast)\n",
    "- Gemma 2B (lightweight)\n",
    "- Qwen 2 (multilingual)\n",
    "\n",
    "### 2. **Prompt Engineering**\n",
    "Test variations:\n",
    "- More explicit instructions\n",
    "- Few-shot examples in the prompt\n",
    "- Chain-of-thought reasoning\n",
    "- Role-playing (e.g., \"You are a psychologist\")\n",
    "\n",
    "### 3. **Validation Improvements**\n",
    "Add stricter Pydantic validators:\n",
    "- Enum constraints for sentiment labels\n",
    "- Range validation for confidence scores\n",
    "- Vocabulary checks for emotions\n",
    "\n",
    "### 4. **Batch Size Tuning**\n",
    "Find optimal throughput:\n",
    "```python\n",
    "batch_sizes = [1, 5, 10, 20]\n",
    "for size in batch_sizes:\n",
    "    time_start = time.time()\n",
    "    # Process 'size' reviews...\n",
    "    time_elapsed = time.time() - time_start\n",
    "    print(f\"Batch {size}: {time_elapsed:.2f}s\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Resources\n",
    "\n",
    "### Ollama Documentation\n",
    "- [Official Docs](https://ollama.com/docs)\n",
    "- [Model Library](https://ollama.com/library)\n",
    "- [API Reference](https://github.com/ollama/ollama/blob/main/docs/api.md)\n",
    "\n",
    "### Prompt Engineering\n",
    "- [OpenAI Prompt Engineering Guide](https://platform.openai.com/docs/guides/prompt-engineering)\n",
    "- [Anthropic Prompt Engineering](https://docs.anthropic.com/claude/docs/prompt-engineering)\n",
    "\n",
    "### Pydantic\n",
    "- [Pydantic Documentation](https://docs.pydantic.dev/)\n",
    "- [Validation Tutorial](https://docs.pydantic.dev/latest/concepts/validators/)\n",
    "\n",
    "### Sentiment Analysis\n",
    "- [VADER Sentiment (compare to your results)](https://github.com/cjhutto/vaderSentiment)\n",
    "- [Hugging Face Sentiment Models](https://huggingface.co/models?pipeline_tag=text-classification&sort=trending)\n",
    "\n",
    "---\n",
    "\n",
    "## Common Troubleshooting\n",
    "\n",
    "| Problem | Solution |\n",
    "|---------|----------|\n",
    "| Ollama not responding | Check if service is running: `ollama serve` |\n",
    "| Slow performance | Try smaller model or enable GPU |\n",
    "| Inconsistent JSON | Make prompt more explicit with examples |\n",
    "| High error rate | Add retries and improve error handling |\n",
    "| Memory issues | Process in smaller batches, free cache regularly |\n",
    "\n",
    "---\n",
    "\n",
    "## Production Deployment Checklist\n",
    "\n",
    "Before deploying to production:\n",
    "\n",
    "- [ ] Test on diverse, representative data samples\n",
    "- [ ] Benchmark performance and resource usage\n",
    "- [ ] Set up monitoring and alerting\n",
    "- [ ] Implement comprehensive error handling\n",
    "- [ ] Add logging for debugging\n",
    "- [ ] Document API/interface clearly\n",
    "- [ ] Plan for model updates and versioning\n",
    "- [ ] Establish data privacy and security measures\n",
    "- [ ] Set up backup and disaster recovery\n",
    "- [ ] Train team members on maintenance\n",
    "\n",
    "---\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "### Short Term (Next Session)\n",
    "1. Run full dataset analysis (remove `.head(3)` limitation)\n",
    "2. Generate summary statistics and visualizations\n",
    "3. Compare results with baseline methods (if available)\n",
    "\n",
    "### Medium Term (This Week)\n",
    "1. Experiment with different models and prompts\n",
    "2. Integrate into existing workflows\n",
    "3. Build a simple dashboard for stakeholders\n",
    "\n",
    "### Long Term (This Month)\n",
    "1. Automate the pipeline with scheduling\n",
    "2. Set up continuous monitoring\n",
    "3. Gather feedback and iterate\n",
    "\n",
    "---\n",
    "\n",
    "## Final Thoughts\n",
    "\n",
    "You now have a **flexible, powerful sentiment analysis system** that:\n",
    "- Runs entirely on your infrastructure\n",
    "- Requires no cloud API keys or costs\n",
    "- Adapts to different datasets automatically\n",
    "- Produces structured, validated output\n",
    "- Scales from prototypes to production\n",
    "\n",
    "The techniques you've learned here apply beyond sentiment analysis:\n",
    "- **Named Entity Recognition** (extract people, places, organizations)\n",
    "- **Text Classification** (categorize documents by topic)\n",
    "- **Summarization** (condense long texts)\n",
    "- **Q&A Systems** (answer questions about documents)\n",
    "- **Content Moderation** (detect inappropriate content)\n",
    "\n",
    "**The foundation you've built is a springboard for countless AI applications!** 🚀\n",
    "\n",
    "---\n",
    "\n",
    "## We'd Love Your Feedback\n",
    "\n",
    "If you found this tutorial helpful:\n",
    "- ⭐ Star the repo on GitHub\n",
    "- 💬 Share your use case in discussions\n",
    "- 🐛 Report issues or suggest improvements\n",
    "- 📚 Contribute additional examples\n",
    "\n",
    "**Happy analyzing!** 🎉"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
