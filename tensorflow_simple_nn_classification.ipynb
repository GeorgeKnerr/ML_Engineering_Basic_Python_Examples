{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow: Simple Neural Network for Classification\n",
    "\n",
    "This example uses TensorFlow with its Keras API to create a simple neural network for binary classification. Classification means assigning input data to one of several categories. Binary classification means there are only two categories (e.g., yes/no, spam/not-spam).\n",
    "\n",
    "We'll use synthetic (artificially generated) data for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f99676",
   "metadata": {},
   "source": [
    "### Prerequisite: Check GPU Availability Using TensorFlow\n",
    "\n",
    "#### Output should look like (complaints can be ignored and only show on 1st run):\n",
    "```bash\n",
    "2025-05-11 20:50:25.015083: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
    "2025-05-11 20:50:25.020710: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
    "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
    "E0000 00:00:1747011025.027882   13580 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
    "E0000 00:00:1747011025.030095   13580 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
    "2025-05-11 20:50:25.037501: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
    "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
    "TensorFlow version: 2.18.0\n",
    "TensorFlow is using 2 GPU(s).\n",
    "TensorFlow built with CUDA version: 12.6\n",
    "TensorFlow built with cuDNN version: 9\n",
    "```\n",
    "#### Output explanation:\n",
    "1. The run logs show that TensorFlow is now imported successfully and is running using 2 GPUs. The lines about registering cuFFT, cuDNN, and cuBLAS are warnings that arise when TensorFlow detects that these libraries have already been registered. They are expected in multi‚ÄêGPU environments and do not indicate a critical error.\n",
    "2. If you prefer to suppress these verbose logs, you can set the environment variable TF_CPP_MIN_LOG_LEVEL to 2 before importing TensorFlow.\n",
    "\n",
    "    ```python\n",
    "    import os\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress INFO and WARNING messages from TensorFlow\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40f5fb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n",
      "TensorFlow is using 2 GPU(s).\n",
      "TensorFlow built with CUDA version: 12.6\n",
      "TensorFlow built with cuDNN version: 9\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import tensorflow as tf\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Error: tensorflow module not found. Please install it via 'pip install tensorflow'\")\n",
    "    raise\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print(f\"TensorFlow is using {len(gpu_devices)} GPU(s).\")\n",
    "    # To get more details about the CUDA version TF was built with:\n",
    "    build_info = tf.sysconfig.get_build_info()\n",
    "    print(f\"TensorFlow built with CUDA version: {build_info.get('cuda_version', 'N/A')}\")\n",
    "    print(f\"TensorFlow built with cuDNN version: {build_info.get('cudnn_version', 'N/A')}\")\n",
    "else:\n",
    "    print(\"TensorFlow: No GPU devices found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Importing Libraries\n",
    "\n",
    "-   `tensorflow` as `tf`: The main TensorFlow library.\n",
    "-   `numpy`: For numerical operations, though TensorFlow has its own tensor operations too.\n",
    "-   `sklearn.datasets.make_classification`: A utility from Scikit-learn to generate a random n-class classification problem. This is great for quickly creating data to test models.\n",
    "-   `sklearn.model_selection.train_test_split`: A utility to split data into training and testing sets. We train the model on the training set and then evaluate its performance on the unseen testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 14:55:50.325349: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-23 14:55:50.372351: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748026550.381811   37919 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748026550.385418   37919 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-23 14:55:50.437970: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  # Missing import for plotting\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler # For feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Generating Synthetic Classification Data\n",
    "\n",
    "-   `make_classification(...)`: Generates the data.\n",
    "    -   `n_samples=1000`: We want 1000 data points (samples).\n",
    "    -   `n_features=4`: Each sample will have 4 input features.\n",
    "    -   `n_classes=2`: There are 2 output classes (binary classification).\n",
    "    -   `random_state=42`: Ensures that we get the same random data every time we run the code, making results reproducible. `42` is just a commonly used number.\n",
    "-   `X` will contain the features (our input data), and `y` will contain the corresponding class labels (0 or 1).\n",
    "-   `train_test_split(X, y, test_size=0.2, random_state=42)`: Splits the data. \n",
    "    -   `test_size=0.2`: Reserves 20% of the data for the test set, and the remaining 80% for the training set.\n",
    "\n",
    "**Feature Scaling**:\n",
    "It's often good practice to scale input features, especially for neural networks. This helps the optimizer work more effectively. We'll use `StandardScaler` from Scikit-learn, which standardizes features by removing the mean and scaling to unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic binary classification data\n",
    "X, y = make_classification(n_samples=1000, n_features=4, n_informative=2, n_redundant=0, \n",
    "                           n_clusters_per_class=1, n_classes=2, random_state=42)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train) # Fit on training data and transform it\n",
    "X_test = scaler.transform(X_test)     # Transform test data using the same scaler\n",
    "\n",
    "print(f\"Shape of X_train: {X_train.shape}, Shape of y_train: {y_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}, Shape of y_test: {y_test.shape}\")\n",
    "print(f\"First 5 samples of X_train (scaled):\\n{X_train[:5]}\")\n",
    "print(f\"First 5 labels of y_train:\\n{y_train[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Defining the Neural Network Model\n",
    "\n",
    "We use Keras's `Sequential` API, which is a straightforward way to build models by stacking layers one after the other.\n",
    "\n",
    "-   `tf.keras.Sequential([...])`: Creates a sequential model.\n",
    "-   `tf.keras.layers.Dense(16, activation='relu', input_shape=(4,))`: The first hidden layer.\n",
    "    -   `Dense`: A fully connected layer, meaning each neuron in this layer is connected to every neuron in the previous layer (or input).\n",
    "    -   `16`: The number of neurons (or units) in this layer.\n",
    "    -   `activation='relu'`: The activation function. 'ReLU' (Rectified Linear Unit) is a common choice for hidden layers. It outputs the input directly if it's positive, otherwise, it outputs zero.\n",
    "    -   `input_shape=(4,)`: Specifies the shape of the input for the first layer only. Our data has 4 features.\n",
    "-   `tf.keras.layers.Dense(8, activation='relu')`: The second hidden layer with 8 neurons and ReLU activation.\n",
    "-   `tf.keras.layers.Dense(1, activation='sigmoid')`: The output layer.\n",
    "    -   `1`: One neuron because this is binary classification (it will output a single value between 0 and 1).\n",
    "    -   `activation='sigmoid'`: The sigmoid activation function squashes the output to a range between 0 and 1. This can be interpreted as the probability of belonging to class 1. For multi-class classification, you'd typically use `softmax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple neural network\n",
    "model_tf = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation='relu', input_shape=(X_train.shape[1],)), # Input shape from X_train\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid') # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Display the model's architecture\n",
    "model_tf.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Compiling the Model\n",
    "\n",
    "Before training, we need to configure the learning process using the `compile` method.\n",
    "\n",
    "-   `optimizer='adam'`: Specifies the optimizer. 'Adam' is another popular and effective optimization algorithm.\n",
    "-   `loss='binary_crossentropy'`: Specifies the loss function. `binary_crossentropy` is suitable for binary classification problems where the output is a probability (due to the sigmoid activation in the last layer).\n",
    "-   `metrics=['accuracy']`: Specifies metrics to monitor during training and evaluation. Here, we want to see the 'accuracy' (the proportion of correctly classified samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model_tf.compile(optimizer='adam', \n",
    "                 loss='binary_crossentropy', \n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Training the Model\n",
    "\n",
    "Now we train the model using the `fit` method.\n",
    "\n",
    "-   `X_train, y_train`: The training data (features and labels).\n",
    "-   `epochs=10`: The number of times the model will see the entire training dataset.\n",
    "-   `batch_size=32`: The number of samples processed before the model's parameters are updated. The training data is divided into batches.\n",
    "-   `validation_split=0.2`: Reserves 20% of the *training data* to be used as validation data. The model's performance on this validation data is monitored during training, which can help detect overfitting (when the model performs well on training data but poorly on unseen data).\n",
    "-   `verbose=1`: How much information to display during training (1 usually means a progress bar and metrics per epoch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"\\nTraining the TensorFlow model...\")\n",
    "history = model_tf.fit(X_train, y_train, \n",
    "                         epochs=10, \n",
    "                         batch_size=32, \n",
    "                         validation_split=0.2, # Use part of training data for validation\n",
    "                         verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 Evaluating the Model\n",
    "\n",
    "After training, we evaluate the model's performance on the test set (`X_test`, `y_test`), which it has never seen before.\n",
    "\n",
    "-   `model.evaluate(X_test, y_test, verbose=0)`: Calculates the loss and any specified metrics (like accuracy) on the test data. `verbose=0` means it won't print progress during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model_tf.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "if 'accuracy' in history.history and 'val_accuracy' in history.history:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Accuracy/loss history not fully available for plotting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 TensorFlow Practice Variation: Add Dropout Layers\n",
    "\n",
    "**Dropout** is a regularization technique used to prevent overfitting in neural networks. Overfitting happens when a model learns the training data too well, including its noise, and performs poorly on new, unseen data.\n",
    "\n",
    "During training, a dropout layer randomly sets a fraction of input units to 0 at each update. This helps to make the network more robust and prevents neurons from co-adapting too much.\n",
    "\n",
    "-   `tf.keras.layers.Dropout(0.2)`: Adds a dropout layer. `0.2` means that 20% of the input units will be randomly set to 0 during each update in the training phase. Dropout is only active during training, not during evaluation or inference.\n",
    "\n",
    "Let's modify our model to include dropout layers after each Dense hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a neural network with Dropout layers\n",
    "model_tf_variation = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dropout(0.3),  # Dropout layer after the first Dense layer (30% dropout rate)\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),  # Dropout layer after the second Dense layer (20% dropout rate)\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Display the new model's architecture\n",
    "model_tf_variation.summary()\n",
    "\n",
    "# Compile the variation model\n",
    "model_tf_variation.compile(optimizer='adam', \n",
    "                           loss='binary_crossentropy', \n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "# Train the variation model\n",
    "print(\"\\nTraining the TensorFlow model with Dropout...\")\n",
    "history_variation = model_tf_variation.fit(X_train, y_train, \n",
    "                                           epochs=10, # Using fewer epochs to see effect quickly, can increase\n",
    "                                           batch_size=32, \n",
    "                                           validation_split=0.2, \n",
    "                                           verbose=1)\n",
    "\n",
    "# Evaluate the variation model\n",
    "test_loss_var, test_accuracy_var = model_tf_variation.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nTest Loss (Variation with Dropout): {test_loss_var:.4f}\")\n",
    "print(f\"Test Accuracy (Variation with Dropout): {test_accuracy_var:.4f}\")\n",
    "\n",
    "# Plot training & validation accuracy and loss for the variation model\n",
    "if 'accuracy' in history_variation.history and 'val_accuracy' in history_variation.history:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history_variation.history['accuracy'], label='Train Acc (Dropout)')\n",
    "    plt.plot(history_variation.history['val_accuracy'], label='Val Acc (Dropout)')\n",
    "    if 'accuracy' in history.history: # Original model's history\n",
    "        plt.plot(history.history['accuracy'], label='Train Acc (Original)', linestyle=':')\n",
    "        plt.plot(history.history['val_accuracy'], label='Val Acc (Original)', linestyle=':')\n",
    "    plt.title('Model Accuracy Comparison')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history_variation.history['loss'], label='Train Loss (Dropout)')\n",
    "    plt.plot(history_variation.history['val_loss'], label='Val Loss (Dropout)')\n",
    "    if 'loss' in history.history: # Original model's history\n",
    "        plt.plot(history.history['loss'], label='Train Loss (Original)', linestyle=':')\n",
    "        plt.plot(history.history['val_loss'], label='Val Loss (Original)', linestyle=':')\n",
    "    plt.title('Model Loss Comparison')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Accuracy/loss history for variation model not fully available for plotting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With dropout, you might see that the training accuracy is a bit lower or increases slower, but the validation accuracy might be better or the gap between training and validation accuracy might be smaller, indicating less overfitting. The effect of dropout is usually more pronounced on more complex models and datasets where overfitting is a bigger risk."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (classification_models_py3.12)",
   "language": "python",
   "name": "classification_models_py3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
