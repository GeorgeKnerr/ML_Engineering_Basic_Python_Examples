{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. OpenAI API Example: Ask GPT-4o a Static Question\n",
    "\n",
    "This example shows how to use the OpenAI API to send a question to the GPT-4o model and get a response. This involves making an HTTP request to OpenAI's servers.\n",
    "\n",
    "**Important Note**: To run this example, you need an OpenAI API key. \n",
    "1.  If you don't have one, sign up at [OpenAI's website](https://platform.openai.com/).\n",
    "2.  Replace `'YOUR_API_KEY'` in the code below with your actual API key.\n",
    "3.  **Keep your API key secret!** Do not share it publicly or commit it to version control.\n",
    "\n",
    "You will also need an active internet connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Importing the OpenAI Library\n",
    "\n",
    "-   `openai`: The official Python library for interacting with the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to install the openai library if you haven't already: !pip install openai\n",
    "import openai\n",
    "from openai import OpenAI # Recommended way to import client\n",
    "import os # To potentially use environment variables for API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Initializing the OpenAI Client\n",
    "\n",
    "We create an instance of the `OpenAI` client, providing our API key.\n",
    "\n",
    "**Security Best Practice**: It's better to store your API key as an environment variable rather than hardcoding it. \n",
    "For example, you could set an environment variable `OPENAI_API_KEY` and then use `api_key=os.environ.get('OPENAI_API_KEY')`.\n",
    "For simplicity in this example, we'll show the direct replacement method, but **be mindful of security**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using API key from environment variable OPENAI_API_KEY.\n",
      "OpenAI client initialized.\n"
     ]
    }
   ],
   "source": [
    "# --- IMPORTANT: REPLACE 'YOUR_API_KEY' WITH YOUR ACTUAL KEY --- \n",
    "# api_key_value = \"YOUR_API_KEY\" \n",
    "# For safety, it's better to use an environment variable or input prompt:\n",
    "try:\n",
    "    # Attempt to get API key from environment variable first\n",
    "    api_key_value = os.environ[\"OPENAI_API_KEY\"]\n",
    "    if not api_key_value:\n",
    "        raise KeyError\n",
    "    print(\"Using API key from environment variable OPENAI_API_KEY.\")\n",
    "except KeyError:\n",
    "    print(\"OPENAI_API_KEY environment variable not found.\")\n",
    "    # Fallback: prompt user for API key if not in environment\n",
    "    # In a real notebook, you might use input() but be careful with visibility\n",
    "    api_key_value = \"YOUR_API_KEY_PLACEHOLDER\" # Replace this or use input()\n",
    "    if api_key_value == \"YOUR_API_KEY_PLACEHOLDER\":\n",
    "         print(\"Please replace 'YOUR_API_KEY_PLACEHOLDER' with your actual key or set the OPENAI_API_KEY environment variable.\")\n",
    "         # You might want to raise an error or skip this section if no key is provided\n",
    "         # For now, we'll let it proceed, but it will fail if the key is invalid.\n",
    "\n",
    "client = None\n",
    "if api_key_value and api_key_value != \"YOUR_API_KEY_PLACEHOLDER\":\n",
    "    try:\n",
    "        client = OpenAI(api_key=api_key_value)\n",
    "        print(\"OpenAI client initialized.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing OpenAI client: {e}\")\n",
    "else:\n",
    "    print(\"OpenAI client NOT initialized. Please provide a valid API key.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Defining the Question and Making the API Request\n",
    "\n",
    "-   `question`: The question we want to ask the model.\n",
    "-   `client.chat.completions.create(...)`: This is the method used to interact with chat-based models like GPT-3.5-turbo, GPT-4, and GPT-4o.\n",
    "    -   `model=\"gpt-4o\"`: Specifies which model to use. \"gpt-4o\" is OpenAI's latest flagship model (as of this writing).\n",
    "    -   `messages=[...]`: A list of message objects that form the conversation history.\n",
    "        -   `{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}`: The system message sets the context or behavior for the assistant.\n",
    "        -   `{\"role\": \"user\", \"content\": question}`: The user's message (our question).\n",
    "    -   `max_tokens=50`: Limits the maximum length of the generated response. One token is roughly 4 characters of text for English.\n",
    "    -   `temperature=0.7`: Controls the randomness of the output. \n",
    "        -   Higher values (e.g., 0.7-1.0) make the output more random and creative.\n",
    "        -   Lower values (e.g., 0.1-0.3) make it more focused and deterministic.\n",
    "        -   A value of 0 would make it mostly deterministic, but not always perfectly so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is the capital city of France?\n",
      "Answer: The capital city of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "if client: # Only proceed if client was initialized\n",
    "    # Define a static question\n",
    "    question = \"What is the capital city of France?\"\n",
    "\n",
    "    try:\n",
    "        # Make a request to GPT-4o\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a concise and helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ],\n",
    "            max_tokens=50, # Maximum number of tokens in the response\n",
    "            temperature=0.7  # Controls randomness: 0.0 (deterministic) to 2.0 (very random)\n",
    "        )\n",
    "\n",
    "        # Extract and print the response\n",
    "        # The response structure is a choice object, we need to access the message content\n",
    "        answer = response.choices[0].message.content\n",
    "        print(f\"\\nQuestion: {question}\")\n",
    "        print(f\"Answer: {answer.strip()}\") # .strip() removes leading/trailing whitespace\n",
    "\n",
    "    except openai.APIConnectionError as e:\n",
    "        print(f\"OpenAI API request failed to connect: {e}\")\n",
    "    except openai.RateLimitError as e:\n",
    "        print(f\"OpenAI API request exceeded rate limit: {e}\")\n",
    "    except openai.AuthenticationError as e:\n",
    "        print(f\"OpenAI API authentication failed. Check your API key: {e}\")\n",
    "    except openai.APIError as e:\n",
    "        print(f\"OpenAI API returned an API Error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "else:\n",
    "    print(\"\\nSkipping OpenAI API call as the client was not initialized (API key missing or invalid).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 OpenAI API Practice Variation: Different Question and Parameters\n",
    "\n",
    "Let's try asking a different question and adjusting the `max_tokens` and `temperature` parameters to see how they affect the output.\n",
    "\n",
    "-   **New Question**: \"Explain the concept of a 'tensor' in machine learning in one sentence.\"\n",
    "-   **`max_tokens`**: We might increase this slightly if we expect a slightly longer answer, or keep it controlled for brevity.\n",
    "-   **`temperature`**: Let's try a lower temperature (e.g., 0.2) for a more deterministic, factual answer, and a higher temperature (e.g., 0.9) for a more creative (though potentially less accurate for factual questions) response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- OpenAI Variation 1: Different Question, Lower Temperature ---\n",
      "Question: Explain the concept of a 'tensor' in machine learning in one or two simple sentences.\n",
      "Answer (temp 0.2): In machine learning, a tensor is a multi-dimensional array used to store data; it can be a scalar (0D), vector (1D), matrix (2D), or higher dimensions, allowing for efficient data manipulation and computation.\n",
      "\n",
      "--- OpenAI Variation 2: Same Question, Higher Temperature ---\n",
      "Question: Explain the concept of a 'tensor' in machine learning in one or two simple sentences.\n",
      "Answer (temp 0.9): In machine learning, a tensor is a multi-dimensional array that generalizes scalars, vectors, and matrices. Tensors are used to represent data, inputs, and parameters in various dimensions, facilitating operations and computations in models.\n"
     ]
    }
   ],
   "source": [
    "if client: # Only proceed if client was initialized\n",
    "    new_question = \"Explain the concept of a 'tensor' in machine learning in one or two simple sentences.\"\n",
    "\n",
    "    print(f\"\\n--- OpenAI Variation 1: Different Question, Lower Temperature ---\")\n",
    "    try:\n",
    "        response_var1 = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful and concise AI assistant, skilled at explaining complex topics simply.\"},\n",
    "                {\"role\": \"user\", \"content\": new_question}\n",
    "            ],\n",
    "            max_tokens=80,  # Slightly more tokens allowed\n",
    "            temperature=0.2   # Lower temperature for more focused response\n",
    "        )\n",
    "        answer_var1 = response_var1.choices[0].message.content\n",
    "        print(f\"Question: {new_question}\")\n",
    "        print(f\"Answer (temp 0.2): {answer_var1.strip()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during OpenAI API call (Variation 1): {e}\")\n",
    "\n",
    "    print(f\"\\n--- OpenAI Variation 2: Same Question, Higher Temperature ---\")\n",
    "    try:\n",
    "        response_var2 = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful and somewhat creative AI assistant.\"}, \n",
    "                {\"role\": \"user\", \"content\": new_question} \n",
    "            ],\n",
    "            max_tokens=80,\n",
    "            temperature=0.9   # Higher temperature for more creative/varied response\n",
    "        )\n",
    "        answer_var2 = response_var2.choices[0].message.content\n",
    "        print(f\"Question: {new_question}\")\n",
    "        print(f\"Answer (temp 0.9): {answer_var2.strip()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during OpenAI API call (Variation 2): {e}\")\n",
    "else:\n",
    "    print(\"\\nSkipping OpenAI API variations as the client was not initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By changing the `temperature`, you'll likely get different phrasings or slightly different explanations for the same question. Lower temperatures are good for factual recall, while higher temperatures can be used for more generative or brainstorming tasks (but be cautious about factual accuracy at very high temperatures)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Further Learning\n",
    "\n",
    "These examples provide a basic introduction to using PyTorch, TensorFlow, and the OpenAI API for common machine learning tasks. \n",
    "\n",
    "**Key Takeaways for Learning & Interviews:**\n",
    "\n",
    "* **Understand the Core Concepts:**\n",
    "    * **PyTorch**: Tensors, `nn.Module` for model building, autograd for gradients, optimizers, and the training loop structure.\n",
    "    * **TensorFlow (Keras)**: The `Sequential` or Functional API for model building, layers, compilation (optimizer, loss, metrics), and the `fit`/`evaluate` workflow.\n",
    "    * **OpenAI API**: Client initialization, structuring messages for chat completions, understanding parameters like `model`, `temperature`, and `max_tokens`, and how to parse the response.\n",
    "* **Practice Explaining Code**: Be able to walk through each part of the code and explain *why* certain choices were made (e.g., why use `relu` vs `sigmoid`, why `binary_crossentropy` for this problem).\n",
    "* **Experiment with Variations**: Actively try modifying these examples. Change architectures, try different optimizers/loss functions, explore other datasets, or ask more complex questions to the OpenAI API. This is the best way to solidify your understanding.\n",
    "* **Setup and Dependencies**: Know how to install the necessary libraries (`pip install ...`) and be aware of potential environment issues (e.g., Python versions, CUDA for GPU).\n",
    "* **Error Handling**: For API calls (like OpenAI), understand common errors (authentication, rate limits, connection issues) and how you might handle them in a real application.\n",
    "\n",
    "This notebook is just a starting point. The field of ML is vast and constantly evolving. Continue exploring documentation, tutorials, and building your own projects to deepen your knowledge!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (classification_models_py3.12)",
   "language": "python",
   "name": "classification_models_py3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
